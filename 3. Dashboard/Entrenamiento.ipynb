{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UN-GCPDS/Curso-Corto-LLMs/blob/main/3.%20Dashboard/Entrenamiento.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Logo UNAL CHEC](https://github.com/UN-GCPDS/curso_IA_CHEC/blob/main/logo_unal_chec.jpg?raw=1)\n",
        "\n",
        "# **Entrenamiento modelo Tabnet**\n",
        "\n",
        "## **Descripción**\n",
        "\n",
        "Entrenamiento de modelo Tabnet bajo diversas condiciones.\n",
        "\n",
        "### **Profesor - Sesión 1:** Andrés Marino Álvarez Meza y Diego Armando Pérez Rosero"
      ],
      "metadata": {
        "id": "mFZXuItQprV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuPas77Kxpn6",
        "outputId": "c401d7c5-f774-43f3-8893-8651eb1dca2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datos\n",
        "\n",
        "**TabNet para criticidad en redes de media tensión — Planteamiento y datos (Regresión)**\n",
        "\n",
        "Sea el conjunto de datos\n",
        "\n",
        "$$\n",
        "\\mathbf{X}\\in\\mathbb{R}^{N\\times M},\\qquad\n",
        "\\mathbf{y}\\in\\mathbb{R}^{N}.\n",
        "$$\n",
        "\n",
        "Cada fila de $\\mathbf{X}$ representa un evento o periodo entre 2019 y 2024 y contiene las características de los elementos asociados al equipo que operó. El vector $\\mathbf{y}$ almacena el valor continuo del indicador a modelar (SAIDI o SAIFI) para ese mismo evento/periodo.\n",
        "\n",
        "Definimos\n",
        "\n",
        "$$\n",
        "\\mathcal{F}:\\mathcal{X}\\subseteq\\mathbb{R}^{M}\\to\\mathbb{R},\\qquad\n",
        "\\hat{y}=\\mathcal{F}(\\mathbf{x})\n",
        "=\n",
        "\\bigl(\\,\\breve{f}_{L}\\circ \\breve{f}_{L-1}\\circ \\cdots \\circ \\breve{f}_{1}\\,\\bigr)(\\mathbf{x}),\n",
        "$$\n",
        "\n",
        "donde $\\breve{f}_{l}(\\cdot)$ denota el $l$-ésimo bloque del modelo ($l\\in\\{1,\\dots,L\\}$) y $\\circ$ es el operador de composición.\n",
        "\n",
        "En caso multisalida para $(\\text{SAIDI},\\text{SAIFI})$, se toma $\\mathcal{F}:\\mathbb{R}^{M}\\to\\mathbb{R}^{2}$ y $\\mathbf{y}\\in\\mathbb{R}^{N\\times 2}$.\n",
        "![Logo UNAL CHEC](https://raw.githubusercontent.com/Daprosero/Deep-Convolutional-Generative-Adversarial-Network/refs/heads/master/Mercados%20CHEC.png)"
      ],
      "metadata": {
        "id": "NHmuwRJcqrHN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9h3KDG32hY8a",
        "outputId": "b3e0ce23-adaa-4708-c773-f49f3566534d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openTSNE\n",
            "  Downloading openTSNE-1.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.12/dist-packages (from openTSNE) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.12/dist-packages (from openTSNE) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from openTSNE) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.20->openTSNE) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.20->openTSNE) (3.6.0)\n",
            "Downloading openTSNE-1.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openTSNE\n",
            "Successfully installed openTSNE-1.0.2\n",
            "Collecting pytorch-tabnet\n",
            "  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from pytorch-tabnet) (2.0.2)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.12/dist-packages (from pytorch-tabnet) (1.6.1)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.12/dist-packages (from pytorch-tabnet) (1.16.1)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.12/dist-packages (from pytorch-tabnet) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.12/dist-packages (from pytorch-tabnet) (4.67.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.16.5)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.43)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (3.19.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.3->pytorch-tabnet) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.3->pytorch-tabnet) (3.0.2)\n",
            "Downloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna, pytorch-tabnet\n",
            "Successfully installed colorlog-6.9.0 optuna-4.5.0 pytorch-tabnet-4.1.0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1o_fZIhk6ErrtrM3eVZPF9s2qj8l4FoqS\n",
            "From (redirected): https://drive.google.com/uc?id=1o_fZIhk6ErrtrM3eVZPF9s2qj8l4FoqS&confirm=t&uuid=3ad67ea6-7bda-40a4-83e1-c50bb830396c\n",
            "To: /content/SuperEventos_Criticidad_AguasAbajo_CODEs.zip\n",
            "100% 214M/214M [00:04<00:00, 47.8MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1lBrseLoEmr6-VwNSCHOp2zuc4sKKrkbQ\n",
            "From (redirected): https://drive.google.com/uc?id=1lBrseLoEmr6-VwNSCHOp2zuc4sKKrkbQ&confirm=t&uuid=11c47b9f-1df7-4f27-ad80-0722b737f7fc\n",
            "To: /content/model.zip\n",
            "100% 109M/109M [00:02<00:00, 46.6MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=16VIuHLgPGpX4J723Wd48UAPhHivLuUaH\n",
            "From (redirected): https://drive.google.com/uc?id=16VIuHLgPGpX4J723Wd48UAPhHivLuUaH&confirm=t&uuid=256ee442-05cd-4685-ad1b-d8d2cc83f748\n",
            "To: /content/Data_CHEC.zip\n",
            "100% 336M/336M [00:07<00:00, 47.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "#@title Librerías\n",
        "# Instalación de paquetes necesarios\n",
        "!pip install -q gdown\n",
        "!pip install openTSNE\n",
        "!pip install pytorch-tabnet optuna\n",
        "!pip install wget --quiet\n",
        "\n",
        "# Importación de librerías necesarias\n",
        "import optuna\n",
        "import warnings\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import softmax\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, QuantileTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor, TabNetClassifier\n",
        "from pytorch_tabnet.augmentations import RegressionSMOTE\n",
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import os\n",
        "from pathlib import Path\n",
        "import math\n",
        "import wget\n",
        "!gdown --id 1o_fZIhk6ErrtrM3eVZPF9s2qj8l4FoqS -O SuperEventos_Criticidad_AguasAbajo_CODEs.zip\n",
        "!gdown --id 1lBrseLoEmr6-VwNSCHOp2zuc4sKKrkbQ -O model.zip\n",
        "!gdown --id 16VIuHLgPGpX4J723Wd48UAPhHivLuUaH -O Data_CHEC.zip\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"SuperEventos_Criticidad_AguasAbajo_CODEs.zip\"\n",
        "extract_dir = \"CHEC\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "zip_path = \"model.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "zip_path = \"Data_CHEC.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "# Supresión de warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"pandas\")\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Función auxiliar para etiquetas\n",
        "def get_labels(x: pd.Series) -> pd.Series:\n",
        "    labels, _ = pd.factorize(x)\n",
        "    return pd.Series(labels, name=x.name, index=x.index)\n",
        "\n",
        "# Definición de funciones personalizadas de pérdida\n",
        "def my_mse_loss_fn(y_pred, y_true):\n",
        "    mse_loss = (y_true - y_pred) ** 2\n",
        "    return torch.mean(mse_loss)\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_var_band(\n",
        "    df,\n",
        "    var_token,\n",
        "    row_index=0,\n",
        "    hours_back=24,\n",
        "    col_patterns=None,\n",
        "    display_name=None,\n",
        "    units=None,\n",
        "    event_label=\"evento reportado\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Grafica una variable climática en una franja de horas hacia atrás.\n",
        "\n",
        "    Parámetros\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Contiene columnas por hora para la variable elegida.\n",
        "        Ejemplos de nombres soportados automáticamente:\n",
        "        - 'h0-<var>', 'h1-<var>', ..., 'h24-<var>'\n",
        "        - '<var>_h0', '<var>_h1', ...\n",
        "        con separadores '_' o '-'.\n",
        "\n",
        "    var_token : str\n",
        "        Nombre base de la variable en los nombres de columna (p.ej. 'wind_gust_spd',\n",
        "        'air_temp', 'precip'). Debe coincidir con lo que aparece en las columnas.\n",
        "\n",
        "    row_index : int\n",
        "        Fila (evento) a graficar.\n",
        "\n",
        "    hours_back : int\n",
        "        Cuántas horas hacia atrás mostrar.\n",
        "\n",
        "    col_patterns : list[str] | None\n",
        "        Lista de regex opcionales para detectar columnas por hora.\n",
        "        Si None, se generan automáticamente a partir de var_token.\n",
        "\n",
        "    display_name : str | None\n",
        "        Etiqueta legible para el eje Y (p.ej. 'Ráfaga de viento').\n",
        "        Si None, se usa var_token.\n",
        "\n",
        "    units : str | None\n",
        "        Unidades para concatenar en la etiqueta Y (p.ej. 'm/s', '°C', 'mm').\n",
        "\n",
        "    event_label : str\n",
        "        Texto para la flecha en la hora 0.\n",
        "    \"\"\"\n",
        "    # --- 1) Preparar patrones de columnas ---\n",
        "    if col_patterns is None:\n",
        "        # Permitir '_' o '-' (o espacio) entre partes del var_token\n",
        "        parts = re.split(r'[_\\-\\s]+', var_token.strip())\n",
        "        # Construimos un regex que tolere '_' o '-' entre partes\n",
        "        # ej: 'wind[_-]?gust[_-]?spd'\n",
        "        var_regex = r'[_-]?'.join(map(re.escape, parts))\n",
        "\n",
        "        col_patterns = [\n",
        "            rf'^h(\\d{{1,2}})[-_]?{var_regex}$',   # h0-<var>  o  h0_<var>\n",
        "            rf'^{var_regex}[-_]?h(\\d{{1,2}})$',   # <var>-h0  o  <var>_h0\n",
        "        ]\n",
        "\n",
        "    # --- 2) Detectar columnas y mapear a hora ---\n",
        "    hour_to_col = {}\n",
        "    for c in df.columns:\n",
        "        for pat in col_patterns:\n",
        "            m = re.match(pat, str(c), flags=re.IGNORECASE)\n",
        "            if m:\n",
        "                h = int(m.group(1))\n",
        "                hour_to_col[h] = c\n",
        "                break\n",
        "\n",
        "    if not hour_to_col:\n",
        "        raise ValueError(\n",
        "            f\"No se encontraron columnas con horas para la variable '{var_token}'.\\n\"\n",
        "            f\"Prueba ajustando 'var_token' o pasando 'col_patterns' personalizados.\"\n",
        "        )\n",
        "\n",
        "    # --- 3) Construir serie horas [0..hours_back] si existen, orden ascendente ---\n",
        "    hours = [h for h in sorted(hour_to_col.keys()) if 0 <= h <= hours_back]\n",
        "    vals = np.array(\n",
        "        [pd.to_numeric(df.loc[df.index[row_index], hour_to_col[h]], errors='coerce') for h in hours],\n",
        "        dtype=float\n",
        "    )\n",
        "\n",
        "    # --- 4) Graficar ---\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "    # línea y puntos\n",
        "    ax.plot(hours, vals, marker='o')\n",
        "\n",
        "    # invertir eje X para que se vea 24 -> 0\n",
        "    ax.set_xlim(hours_back, 0)\n",
        "\n",
        "    # franja sombreada\n",
        "    ymin = np.nanmin(vals) if np.isfinite(np.nanmin(vals)) else 0.0\n",
        "    ymax = np.nanmax(vals) if np.isfinite(np.nanmax(vals)) else 1.0\n",
        "    pad  = 0.05 * (ymax - ymin if ymax > ymin else 1.0)\n",
        "    ax.set_ylim(ymin - pad, ymax + pad)\n",
        "    ax.axvspan(0, hours_back, alpha=0.15)\n",
        "\n",
        "    # flecha y etiqueta en hora 0\n",
        "    y0 = vals[hours.index(0)] if 0 in hours else np.nan\n",
        "    if not np.isfinite(y0):\n",
        "        y0 = np.nanmedian(vals) if np.isfinite(np.nanmedian(vals)) else (ymin + ymax) / 2.0\n",
        "\n",
        "    ax.annotate(\n",
        "        event_label,\n",
        "        xy=(0, y0),\n",
        "        xytext=(max(2, min(4, hours_back*0.15)), y0 + (ymax - y0)*0.15),\n",
        "        arrowprops=dict(arrowstyle=\"->\", lw=1),\n",
        "        ha='left', va='bottom'\n",
        "    )\n",
        "\n",
        "    # etiquetas\n",
        "    ylab = display_name if display_name else var_token\n",
        "    if units:\n",
        "        ylab = f\"{ylab} [{units}]\"\n",
        "    ax.set_xlabel(\"Horas antes del evento\")\n",
        "    ax.set_ylabel(ylab)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # ticks principales (24, 18, 12, 6, 0) si corresponde\n",
        "    xticks = [h for h in [hours_back, 18, 12, 6, 0] if 0 <= h <= hours_back]\n",
        "    ax.set_xticks(xticks)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# --- Ejemplo de uso:\n",
        "# plot_wind_gust_band(df=tu_dataframe, row_index=0, hours_back=24)\n",
        "\n",
        "def my_rmse_loss_fn(y_pred, y_true):\n",
        "    mse_loss = (y_true - y_pred) ** 2\n",
        "    mean_mse_loss = torch.mean(mse_loss)\n",
        "    rmse_loss = torch.sqrt(mean_mse_loss)\n",
        "    return rmse_loss\n",
        "\n",
        "def my_mae_loss_fn(y_pred, y_true):\n",
        "    mae_loss = torch.abs(y_true - y_pred)\n",
        "    return torch.mean(mae_loss)\n",
        "\n",
        "def my_mape_loss_fn(y_pred, y_true):\n",
        "    mape_loss = torch.abs((y_true - y_pred) / y_true) * 100\n",
        "    return torch.mean(mape_loss)\n",
        "\n",
        "def my_r2_score_fn(y_pred, y_true):\n",
        "    total_variance = torch.var(y_true, unbiased=False)\n",
        "    unexplained_variance = torch.mean((y_true - y_pred) ** 2)\n",
        "    r2_score = 1 - (unexplained_variance / total_variance)\n",
        "    return 1-r2_score\n",
        "\n",
        "# Etapa 0: imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "# ==== Librerías ====\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "import xgboost as xgb\n",
        "\n",
        "from cuml.ensemble import RandomForestRegressor as cuRF\n",
        "from cuml.metrics import r2_score as r2_gpu\n",
        "\n",
        "# Si quieres comparar con CPU para sanity-check:\n",
        "from sklearn.metrics import r2_score as r2_cpu\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "# ==== Utilidades ====\n",
        "def to_cpu(a):\n",
        "    \"\"\"Convierte CuPy -> NumPy si aplica.\"\"\"\n",
        "    try:\n",
        "        if isinstance(a, cp.ndarray):\n",
        "            return cp.asnumpy(a)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return a\n",
        "\n",
        "def metrics_gpu(y_true_cp, y_pred_cp):\n",
        "    \"\"\"MAE, RMSE, R2 calculados en GPU (CuPy).\"\"\"\n",
        "    y_true_cp = cp.asarray(y_true_cp)\n",
        "    y_pred_cp = cp.asarray(y_pred_cp)\n",
        "    mae  = float(cp.mean(cp.abs(y_true_cp - y_pred_cp)))\n",
        "    rmse = float(cp.sqrt(cp.mean((y_true_cp - y_pred_cp)**2)))\n",
        "    ssr  = float(cp.sum((y_true_cp - y_pred_cp)**2))\n",
        "    sst  = float(cp.sum((y_true_cp - cp.mean(y_true_cp))**2))\n",
        "    r2   = 1.0 - ssr / sst if sst > 0 else np.nan\n",
        "    return mae, rmse, r2\n",
        "\n",
        "def permutation_importance_rf_gpu(model, X_val_cp, y_val_cp, n_repeats=3, max_feats=None, random_state=42):\n",
        "    \"\"\"\n",
        "    Permutation importance en GPU para RF cuML.\n",
        "    Devuelve importancia por feature (drop medio de R2 en valid).\n",
        "    Si max_feats no es None, calcula solo para las primeras max_feats columnas (para acelerar).\n",
        "    \"\"\"\n",
        "    rs = cp.random.RandomState(random_state)\n",
        "    X_val_cp = cp.asarray(X_val_cp)\n",
        "    y_val_cp = cp.asarray(y_val_cp)\n",
        "\n",
        "    # R2 base\n",
        "    y_pred_base = model.predict(X_val_cp)\n",
        "    _, _, r2_base = metrics_gpu(y_val_cp, y_pred_base)\n",
        "\n",
        "    n, d = X_val_cp.shape\n",
        "    d_eval = d if max_feats is None else int(min(max_feats, d))\n",
        "    importances = cp.zeros(d, dtype=cp.float32)\n",
        "\n",
        "    for j in range(d_eval):\n",
        "        drops = []\n",
        "        for _ in range(n_repeats):\n",
        "            Xp = X_val_cp.copy()\n",
        "            idx = rs.permutation(n)\n",
        "            Xp[:, j] = Xp[idx, j]  # permutar solo la columna j\n",
        "            y_pred_p = model.predict(Xp)\n",
        "            _, _, r2_p = metrics_gpu(y_val_cp, y_pred_p)\n",
        "            drops.append(r2_base - r2_p)\n",
        "        importances[j] = cp.mean(cp.asarray(drops))\n",
        "\n",
        "    return importances  # CuPy array\n",
        "def regression_metrics(y_true, y_pred):\n",
        "    mae  = float(np.mean(np.abs(y_true - y_pred)))\n",
        "    rmse = float(np.sqrt(np.mean((y_true - y_pred)**2)))\n",
        "    ss_res = float(np.sum((y_true - y_pred)**2))\n",
        "    ss_tot = float(np.sum((y_true - np.mean(y_true))**2))\n",
        "    r2 = 1 - ss_res/ss_tot if ss_tot > 0 else np.nan\n",
        "    return mae, rmse, r2\n",
        "class CustomTabNetRegressor(TabNetRegressor):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(CustomTabNetRegressor, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, X):\n",
        "        output, M_loss = self.network(X)\n",
        "        output = torch.relu(output)\n",
        "        return output, M_loss\n",
        "\n",
        "    def predict(self, X):\n",
        "        device = next(self.network.parameters()).device\n",
        "        if not isinstance(X, torch.Tensor):\n",
        "            X = torch.tensor(X, dtype=torch.float32)\n",
        "        X = X.to(device)\n",
        "        with torch.no_grad():\n",
        "            output, _ = self.forward(X)\n",
        "        return output.cpu().numpy()\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.spatial import cKDTree\n",
        "from tqdm import tqdm\n",
        "from ast import literal_eval\n",
        "from pandas.api.types import is_numeric_dtype\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "def make_strat_labels(y_vals, n_bins=3, min_per_class=2):\n",
        "    \"\"\"\n",
        "    Genera etiquetas para estratificar a partir de un objetivo continuo.\n",
        "    Reduce bins si no hay suficientes muestras por clase.\n",
        "    \"\"\"\n",
        "    y1d = y_vals.reshape(-1)\n",
        "    for bins in range(n_bins, 1, -1):\n",
        "        pct = np.linspace(0, 100, bins + 1)[1:-1]\n",
        "        cuts = np.percentile(y1d, pct)\n",
        "        if np.any(np.diff(cuts) <= 0):\n",
        "            continue\n",
        "        labels = np.digitize(y1d, bins=cuts).astype(int)\n",
        "        counts = Counter(labels)\n",
        "        if all(c >= min_per_class for c in counts.values()) and len(counts) > 1:\n",
        "            return labels\n",
        "    return None\n",
        "\n",
        "def stratify_from_df_or_y(df_labels, idx, y_subset, col='NIVEL_C'):\n",
        "    \"\"\"Intenta usar df[col] como etiqueta; si falla, usa percentiles en y_subset.\"\"\"\n",
        "    try:\n",
        "        ycat_full = df_labels.loc[:, col].values.astype(int)\n",
        "        ycat = ycat_full[idx]\n",
        "        c10 = Counter(ycat)\n",
        "        if all(v >= 2 for v in c10.values()) and len(c10) > 1:\n",
        "            return ycat\n",
        "    except Exception:\n",
        "        pass\n",
        "    return make_strat_labels(y_subset[:,0], n_bins=3, min_per_class=2)\n",
        "\n",
        "def split_subset(X, y, df_labels=None, n_sub=1000, test_size=0.20, seed=42):\n",
        "    \"\"\"\n",
        "    1) Toma un subset aleatorio de tamaño n_sub.\n",
        "    2) Escala y (MinMax) sobre el subset.\n",
        "    3) Split train/test con estratificación si es viable.\n",
        "    4) Split train/valid (20% del train), con re-estratificación si es posible.\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(seed)\n",
        "    n_total = X.shape[0]\n",
        "    n_sub = min(n_sub, n_total)\n",
        "    idx_sub = rng.choice(n_total, size=n_sub, replace=False)\n",
        "\n",
        "    X_sub = X[idx_sub]\n",
        "    y_sub = y[idx_sub]\n",
        "    # etiquetas auxiliares para estratificación\n",
        "    ycat_sub = stratify_from_df_or_y(df_labels, idx_sub, y_sub) if df_labels is not None else make_strat_labels(y_sub[:,0])\n",
        "    # escalar objetivo en el subset\n",
        "    scaler = MinMaxScaler()\n",
        "    y_sub_scaled = scaler.fit_transform(y_sub)\n",
        "\n",
        "    split_kwargs = dict(test_size=test_size, random_state=seed, shuffle=True)\n",
        "    if ycat_sub is not None:\n",
        "        X_tr, X_te, y_tr, y_te, ycat_tr, ycat_te = train_test_split(\n",
        "            X_sub, y_sub_scaled, ycat_sub, stratify=ycat_sub, **split_kwargs\n",
        "        )\n",
        "    else:\n",
        "        X_tr, X_te, y_tr, y_te = train_test_split(X_sub, y_sub_scaled, **split_kwargs)\n",
        "        ycat_tr = ycat_te = None\n",
        "\n",
        "    # Validación (20% del train)\n",
        "    if ycat_tr is not None:\n",
        "        y_tr_raw = y_tr[:,0]\n",
        "        ycat_t = make_strat_labels(y_tr_raw, n_bins=3, min_per_class=2)\n",
        "        if ycat_t is not None:\n",
        "            X_tr, X_va, y_tr, y_va, ycat_tr, ycat_va = train_test_split(\n",
        "                X_tr, y_tr, ycat_tr, test_size=0.20, random_state=seed, stratify=ycat_t\n",
        "            )\n",
        "        else:\n",
        "            X_tr, X_va, y_tr, y_va = train_test_split(\n",
        "                X_tr, y_tr, test_size=0.20, random_state=seed, shuffle=True\n",
        "            )\n",
        "            ycat_va = None\n",
        "    else:\n",
        "        X_tr, X_va, y_tr, y_va = train_test_split(\n",
        "            X_tr, y_tr, test_size=0.20, random_state=seed, shuffle=True\n",
        "        )\n",
        "        ycat_va = None\n",
        "\n",
        "    # Reporte rápido\n",
        "    print(\"Originales (conservados):\", X_orig.shape, y_orig.shape)\n",
        "    print(f\"Subset de {n_sub}:\", X_sub.shape, y_sub.shape)\n",
        "    print(\"Train/Valid/Test:\", X_tr.shape, X_va.shape, X_te.shape)\n",
        "    if ycat_sub is not None:\n",
        "        print(\"Distribución clases subset:\", Counter(ycat_sub))\n",
        "\n",
        "    return {\n",
        "        \"idx_sub\": idx_sub,\n",
        "        \"X_train\": X_tr, \"X_valid\": X_va, \"X_test\": X_te,\n",
        "        \"y_train\": y_tr, \"y_valid\": y_va, \"y_test\": y_te\n",
        "    }\n",
        "from copy import deepcopy\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "def make_tabnet(cat_info, params):\n",
        "    cat_idxs = [i for i, f in enumerate(features) if f in CATEGORICAL_COLUMNS]\n",
        "    cat_dims = [categorical_dims[f] for f in features if f in CATEGORICAL_COLUMNS]\n",
        "    cat_emb_dim = [min(params['emb'], max(4, (dim + 1)//2)) for dim in cat_dims]\n",
        "    return cat_idxs, cat_dims, cat_emb_dim\n",
        "\n",
        "def build_optimizer(optimizer_type, learning_rate, momentum, weight_decay):\n",
        "    if optimizer_type == 'adam':\n",
        "        optimizer_fn = torch.optim.Adam\n",
        "        optimizer_params = {'lr': learning_rate, 'weight_decay': weight_decay}\n",
        "    elif optimizer_type == 'adamw':\n",
        "        optimizer_fn = torch.optim.AdamW\n",
        "        optimizer_params = {'lr': learning_rate, 'weight_decay': weight_decay}\n",
        "    elif optimizer_type == 'sgd':\n",
        "        optimizer_fn = torch.optim.SGD\n",
        "        optimizer_params = {'lr': learning_rate, 'momentum': momentum, 'weight_decay': weight_decay}\n",
        "    elif optimizer_type == 'rmsprop':\n",
        "        optimizer_fn = torch.optim.RMSprop\n",
        "        optimizer_params = {'lr': learning_rate, 'momentum': momentum, 'weight_decay': weight_decay}\n",
        "    return optimizer_fn, optimizer_params\n",
        "def objective_regression(trial):\n",
        "    # Capacidad TabNet\n",
        "    n_d     = trial.suggest_int('n_d', 2, 256)\n",
        "    n_a     = trial.suggest_int('n_a', 2, 256)\n",
        "    n_steps = trial.suggest_int('n_steps', 1, 10)\n",
        "\n",
        "    gamma         = trial.suggest_float('gamma', 1e-12, 1e2)\n",
        "    lambda_sparse = trial.suggest_float('lambda_sparse', 1e-12, 1e2, log=True)\n",
        "\n",
        "    batch_size  = trial.suggest_categorical('batch_size', [1024, 2048, 4096])\n",
        "    mask_type   = trial.suggest_categorical('mask_type', ['entmax', 'sparsemax'])\n",
        "    emb         = trial.suggest_int('emb', 3, 70)\n",
        "\n",
        "    momentum      = trial.suggest_float('momentum', 0.001, 0.9)\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-7, 963e-1, log=True)\n",
        "    weight_decay  = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
        "\n",
        "    scheduler_gamma = trial.suggest_float('scheduler_gamma', 0.1, 0.99)\n",
        "    step_size       = trial.suggest_int('step_size',  1, 20)\n",
        "\n",
        "    virtual_batch_size = trial.suggest_categorical('virtual_batch_size', [256,512,1024])\n",
        "    if isinstance(batch_size, int) and isinstance(virtual_batch_size, int) and virtual_batch_size > batch_size:\n",
        "        virtual_batch_size = batch_size // 2 if batch_size >= 64 else batch_size\n",
        "\n",
        "    optimizer_type = trial.suggest_categorical('optimizer_type', ['adam', 'adamw', 'sgd', 'rmsprop'])\n",
        "    optimizer_fn, optimizer_params = build_optimizer(optimizer_type, learning_rate, momentum, weight_decay)\n",
        "\n",
        "    p   = trial.suggest_float('p', 1e-6, 0.99)\n",
        "    aug = RegressionSMOTE(p=p)\n",
        "\n",
        "    cat_idxs = [i for i, f in enumerate(features) if f in CATEGORICAL_COLUMNS]\n",
        "    cat_dims = [categorical_dims[f] for f in features if f in CATEGORICAL_COLUMNS]\n",
        "    cat_emb_dim = [min(emb, max(4, (dim + 1)//2)) for dim in cat_dims]\n",
        "\n",
        "    model = CustomTabNetRegressor(\n",
        "        cat_dims=cat_dims, cat_emb_dim=cat_emb_dim, cat_idxs=cat_idxs,\n",
        "        n_d=n_d, n_a=n_a, n_steps=n_steps, gamma=gamma, lambda_sparse=lambda_sparse,\n",
        "        mask_type=mask_type, optimizer_fn=optimizer_fn, optimizer_params=optimizer_params,\n",
        "        scheduler_params={\"gamma\": scheduler_gamma, \"step_size\": step_size},\n",
        "        scheduler_fn=torch.optim.lr_scheduler.StepLR, verbose=False\n",
        "    )\n",
        "    model.fit(\n",
        "        X_train=X_train, y_train=y_train,\n",
        "        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
        "        eval_name=['train', 'valid'],\n",
        "        eval_metric=['mae'],\n",
        "        loss_fn=my_r2_score_fn,  # (conserva tu lógica)\n",
        "        max_epochs=70, patience=40,\n",
        "        batch_size=batch_size, virtual_batch_size=virtual_batch_size,\n",
        "        num_workers=1, drop_last=False, augmentations=aug,\n",
        "    )\n",
        "    mae = model.history['loss'][-1]\n",
        "    return mae\n",
        "\n",
        "def eval_and_print(title, clf_model, X_test, y_test):\n",
        "    \"\"\"Evalúa R² en escala original (inverse_transform) y lo imprime.\"\"\"\n",
        "    y_pred = clf_model.predict(X_test)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    print(f\"{title}: R2={r2:.4f}\")\n",
        "    return r2\n",
        "\n",
        "def run_three_training_strategies(\n",
        "    # modelos / kwargs\n",
        "    clf_base,                   # modelo ya entrenado en la Fase 1 (con warm_start=True)\n",
        "    model_init_kwargs,          # dict con los kwargs para construir un modelo nuevo idéntico (desde cero)\n",
        "    # datos antiguos (Fase 1)\n",
        "    X_train_old, y_train_old,   # típicamente (X_train, y_train[:,0:1]) de los 1000\n",
        "    X_test_old, y_test_old,  # test y scaler usados en la Fase 1\n",
        "    # datos nuevos (Fase 2)\n",
        "    X_tr_new, y_tr_new,         # train de los 500\n",
        "    X_va_new, y_va_new,         # valid de los 500 (para early stopping)\n",
        "    X_te_new, y_te_new,  # test nuevo y su scaler\n",
        "    # entrenamiento\n",
        "    batch_size, virtual_batch_size, aug,\n",
        "    max_epochs_ft_inc=200, patience_ft_inc=70,\n",
        "    max_epochs_ft_new=200, patience_ft_new=70,\n",
        "    max_epochs_scratch=200, patience_scratch=70,\n",
        "    lower_lr_factor=0.1, min_lr=1e-5\n",
        "):\n",
        "    \"\"\"\n",
        "    Ejecuta:\n",
        "      A) Fine-tuning incremental (old + new)\n",
        "      B) Fine-tuning no incremental (solo new)\n",
        "      C) Desde cero (old + new)\n",
        "    y evalúa R² en test viejo y test nuevo (ambos en escala original).\n",
        "    Devuelve un dict con los R².\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "\n",
        "    # =============================\n",
        "    # A) Fine-tuning incremental\n",
        "    # =============================\n",
        "    clf_ft_inc = deepcopy(clf_base)  # copia del clf ya entrenado\n",
        "    # bajar LR para fine-tune (opcional, recomendado)\n",
        "    if hasattr(clf_ft_inc, \"_optimizer\"):\n",
        "        for g in clf_ft_inc._optimizer.param_groups:\n",
        "            g[\"lr\"] = max(g[\"lr\"] * lower_lr_factor, min_lr)\n",
        "\n",
        "    X_inc = np.concatenate([X_train_old, X_tr_new], axis=0)\n",
        "    y_inc = np.concatenate([y_train_old, y_tr_new], axis=0)\n",
        "\n",
        "    clf_ft_inc.fit(\n",
        "        X_train=X_inc, y_train=y_inc,\n",
        "        eval_set=[(X_inc, y_inc), (X_va_new, y_va_new)],\n",
        "        eval_name=['train_inc', 'valid_new'],\n",
        "        eval_metric=['mae'], loss_fn=my_r2_score_fn,\n",
        "        max_epochs=max_epochs_ft_inc, patience=patience_ft_inc,\n",
        "        batch_size=batch_size, virtual_batch_size=virtual_batch_size,\n",
        "        num_workers=1, drop_last=False, augmentations=aug,\n",
        "    )\n",
        "\n",
        "    print(\"\\n== Desempeño: Fine-tuning incremental ==\")\n",
        "    r2_old_inc = eval_and_print(\"Test viejo (FT incremental)\", clf_ft_inc, X_test_old, y_test_old)\n",
        "    r2_new_inc = eval_and_print(\"Test nuevo (FT incremental)\", clf_ft_inc, X_te_new,  y_te_new)\n",
        "    results[\"fine_tune_incremental\"] = {\"R2_old_test\": r2_old_inc, \"R2_new_test\": r2_new_inc, \"model\": clf_ft_inc}\n",
        "\n",
        "    # =============================\n",
        "    # B) Fine-tuning no incremental (solo nuevos)\n",
        "    # =============================\n",
        "    clf_ft_new = deepcopy(clf_base)\n",
        "    if hasattr(clf_ft_new, \"_optimizer\"):\n",
        "        for g in clf_ft_new._optimizer.param_groups:\n",
        "            g[\"lr\"] = max(g[\"lr\"] * lower_lr_factor, min_lr)\n",
        "\n",
        "    clf_ft_new.fit(\n",
        "        X_train=X_tr_new, y_train=y_tr_new,\n",
        "        eval_set=[(X_tr_new, y_tr_new), (X_va_new, y_va_new)],\n",
        "        eval_name=['train_new', 'valid_new'],\n",
        "        eval_metric=['mae'], loss_fn=my_r2_score_fn,\n",
        "        max_epochs=max_epochs_ft_new, patience=patience_ft_new,\n",
        "        batch_size=batch_size, virtual_batch_size=virtual_batch_size,\n",
        "        num_workers=1, drop_last=False, augmentations=aug,\n",
        "    )\n",
        "\n",
        "    print(\"\\n== Desempeño: Fine-tuning NO incremental (solo nuevos) ==\")\n",
        "    r2_old_new = eval_and_print(\"Test viejo (FT no incremental)\", clf_ft_new, X_test_old, y_test_old)\n",
        "    r2_new_new = eval_and_print(\"Test nuevo (FT no incremental)\", clf_ft_new, X_te_new,  y_te_new)\n",
        "    results[\"fine_tune_only_new\"] = {\"R2_old_test\": r2_old_new, \"R2_new_test\": r2_new_new, \"model\": clf_ft_new}\n",
        "\n",
        "    # =============================\n",
        "    # C) Desde cero (cumulative old+new)\n",
        "    # =============================\n",
        "    # model_init_kwargs debe contener todo lo necesario para reconstruir el TabNet\n",
        "    clf_scratch = CustomTabNetRegressor(**model_init_kwargs)\n",
        "\n",
        "    X_cum = np.concatenate([X_train_old, X_tr_new], axis=0)\n",
        "    y_cum = np.concatenate([y_train_old, y_tr_new], axis=0)\n",
        "\n",
        "    clf_scratch.fit(\n",
        "        X_train=X_cum, y_train=y_cum,\n",
        "        eval_set=[(X_cum, y_cum), (X_va_new, y_va_new)],\n",
        "        eval_name=['train_cum', 'valid_new'],\n",
        "        eval_metric=['mae'], loss_fn=my_r2_score_fn,\n",
        "        max_epochs=max_epochs_scratch, patience=patience_scratch,\n",
        "        batch_size=batch_size, virtual_batch_size=virtual_batch_size,\n",
        "        num_workers=1, drop_last=False, augmentations=aug,\n",
        "    )\n",
        "\n",
        "    print(\"\\n== Desempeño: Desde cero (old+new) ==\")\n",
        "    r2_old_sc = eval_and_print(\"Test viejo (desde cero)\", clf_scratch, X_test_old, y_test_old)\n",
        "    r2_new_sc = eval_and_print(\"Test nuevo (desde cero)\", clf_scratch, X_te_new,  y_te_new)\n",
        "    results[\"from_scratch\"] = {\"R2_old_test\": r2_old_sc, \"R2_new_test\": r2_new_sc, \"model\": clf_scratch}\n",
        "    return results\n",
        "def pick_new_indices(n_new=500, seed=123):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    universe = np.setdiff1d(np.arange(X.shape[0]), splits_1000[\"idx_sub\"], assume_unique=True)\n",
        "    n_new = min(n_new, universe.shape[0])\n",
        "    return rng.choice(universe, size=n_new, replace=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Xdata = df = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Diego/SuperEventos_Criticidad_AguasAbajo_CODEs.pkl')\n",
        "Xdata = Xdata[Xdata['duracion_h'] <= 100]\n",
        "# ---------------------------------------------------------\n",
        "# Etapa 1: seleccionar objetivo (SAIDI o SAIFI) con forma (N,1)\n",
        "# Extraer variables objetivo\n",
        "Dur_h = Xdata['duracion_h'].values\n",
        "SAIDI = Xdata['SAIDI'].values\n",
        "df1=Xdata.copy()\n",
        "# Eliminar columnas no utilizadas\n",
        "Xdata.drop(['inicio_evento', 'h0-solar_rad', 'h0-uv', 'h1-solar_rad', 'h1-uv', 'h2-solar_rad', 'h2-uv', 'h3-solar_rad', 'h3-uv',\n",
        "            'h4-solar_rad', 'h4-uv', 'h5-solar_rad', 'h5-uv', 'h19-solar_rad', 'h19-uv', 'h20-solar_rad', 'h20-uv',\n",
        "            'h21-solar_rad', 'h21-uv', 'h22-solar_rad', 'h22-uv', 'h23-solar_rad', 'h23-uv', 'evento', 'fin', 'inicio',\n",
        "            'cnt_usus', 'DEP', 'MUN', 'FECHA', 'NIVEL_C', 'VALOR_C', 'TRAMOS_AGUAS_ABAJO', 'EQUIPOS_PUNTOS',\n",
        "            'PUNTOS_POLIGONO', 'LONGITUD2', 'LATITUD2', 'FECHA_C','TRAMOS_AGUAS_ABAJO_CODES','ORDER_'],\n",
        "           inplace=True, axis=1)\n",
        "\n",
        "# Definir la variable objetivo y eliminarla del conjunto de características\n",
        "target = ['SAIFI', 'SAIDI', 'duracion_h']\n",
        "y1 = Xdata[target].values\n",
        "Xdata.drop(target, axis=1, inplace=True)\n",
        "y = y1[:, 0:1].astype('float32')\n",
        "\n",
        "# Copia de trabajo de X\n",
        "df = Xdata.copy()\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Etapa 2: tipificar columnas\n",
        "NUMERIC_COLUMNS = df.select_dtypes(include=['number']).columns.tolist()\n",
        "CATEGORICAL_COLUMNS = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Etapa 3: imputación numérica\n",
        "max_values = {}\n",
        "for col in NUMERIC_COLUMNS:\n",
        "    max_value = pd.to_numeric(df[col], errors='coerce').max()\n",
        "    if pd.isna(max_value):\n",
        "        max_value = 0.0\n",
        "    max_values[col] = max_value\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(-10.0 * max_value)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Etapa 4: codificación categórica\n",
        "label_encoders = {}\n",
        "categorical_dims = {}\n",
        "for col in CATEGORICAL_COLUMNS:\n",
        "    enc = LabelEncoder()\n",
        "    s = df[col].astype(str).fillna(\"no aplica\")\n",
        "    enc.fit(s)\n",
        "    df[col] = enc.transform(s)\n",
        "    label_encoders[col] = enc\n",
        "    categorical_dims[col] = len(enc.classes_)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Etapa 5: construir matrices X, y\n",
        "unused_feat = []\n",
        "# Si Xdata NO incluye el target, basta con tomar todas las columnas\n",
        "features = [c for c in df.columns if c not in unused_feat]\n",
        "X = df[features].values.astype('float32')\n",
        "# Etapa 6: clases auxiliares para estratificación\n",
        "try:\n",
        "    # usar etiqueta externa si existe\n",
        "    y_categorized = df1['NIVEL_C'].values.astype(int)\n",
        "except Exception:\n",
        "    # fallback: terciles del objetivo\n",
        "    percentiles = np.percentile(y[:, 0], [33.33, 66.66])\n",
        "    y_categorized = np.digitize(y[:, 0].flatten(), bins=percentiles).astype(int)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Etapa 7: escalar objetivo (regresión)\n",
        "scaler = MinMaxScaler()\n",
        "y_scaled = scaler.fit_transform(y)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Etapa 8: split train/test estratificado\n",
        "X_train, X_test, y_train, y_test, ycat_train, ycat_test = train_test_split(\n",
        "    X, y_scaled, y_categorized, test_size=0.20, random_state=42, stratify=y_categorized\n",
        ")\n",
        "\n",
        "# Etapa 8b: split train/valid estratificado por percentiles de y_train\n",
        "percentiles_t = np.percentile(y_train[:, 0], [25, 50, 75])\n",
        "y_categorized_t = np.digitize(y_train[:, 0].flatten(), bins=percentiles_t).astype(int)\n",
        "\n",
        "X_train, X_valid, y_train, y_valid, ycat_train, ycat_valid = train_test_split(\n",
        "    X_train, y_train, ycat_train, test_size=0.20, random_state=42, stratify=y_categorized_t\n",
        ")\n",
        "\n",
        "# Comprobaciones rápidas\n",
        "print(X.shape, y.shape)\n",
        "print(\"Train/Valid/Test:\", X_train.shape, X_valid.shape, X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lvp4foXlErX",
        "outputId": "c1327f64-fc19-47b5-edc7-147575700e0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(166323, 326) (166323, 1)\n",
            "Train/Valid/Test: (106446, 326) (26612, 326) (33265, 326)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Logo UNAL CHEC](https://miro.medium.com/v2/resize:fit:1100/format:webp/0*lu62RCEko0VYe-YZ)\n",
        "\n"
      ],
      "metadata": {
        "id": "0mwgoq6ePRAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import optuna\n",
        "# study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler())\n",
        "\n",
        "# study.optimize(objective_regression, n_trials=15)\n",
        "\n",
        "\n",
        "import optuna\n",
        "from tqdm import tqdm\n",
        "\n",
        "n_trials = 20  # o el número que quieras\n",
        "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler())\n",
        "\n",
        "# tqdm envolviendo el bucle de optimización\n",
        "with tqdm(total=n_trials) as pbar:\n",
        "    def callback(study, trial):\n",
        "        pbar.update(1)  # avanza una barra por cada trial completado\n",
        "\n",
        "    study.optimize(objective_regression, n_trials=n_trials, callbacks=[callback])\n",
        "\n",
        "print(\"Best hyperparameters for regression: \", study.best_params)\n",
        "print(\"Best mae: \", study.best_value)\n",
        "par = study.best_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDxnDSJNSnAI",
        "outputId": "e9d7d6ae-7245-4067-fc73-c835accaff20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-09-11 16:46:45,023] A new study created in memory with name: no-name-b36100df-99c3-4c0f-a143-af05b801d053\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 70 with best_epoch = 58 and best_valid_mae = 0.61356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "[I 2025-09-11 16:52:53,386] Trial 0 finished with value: 16837.685604709488 and parameters: {'n_d': 85, 'n_a': 121, 'n_steps': 3, 'gamma': 37.817878183857346, 'lambda_sparse': 2.6906000765077263e-09, 'batch_size': 4096, 'mask_type': 'sparsemax', 'emb': 37, 'momentum': 0.6630316518136158, 'learning_rate': 5.161876035583859e-05, 'weight_decay': 1.8061471155327178e-05, 'scheduler_gamma': 0.5155048083306591, 'step_size': 12, 'virtual_batch_size': 1024, 'optimizer_type': 'adam', 'p': 0.09355034491413258}. Best is trial 0 with value: 16837.685604709488.\n",
            "  5%|▌         | 1/20 [06:08<1:56:38, 368.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 70 with best_epoch = 46 and best_valid_mae = 0.06251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "[I 2025-09-11 16:57:59,350] Trial 1 finished with value: 397.5173647525249 and parameters: {'n_d': 2, 'n_a': 203, 'n_steps': 1, 'gamma': 24.108317895090796, 'lambda_sparse': 0.002045087321360193, 'batch_size': 4096, 'mask_type': 'sparsemax', 'emb': 54, 'momentum': 0.697491258595262, 'learning_rate': 2.869737037567972e-07, 'weight_decay': 1.7516285429780346e-05, 'scheduler_gamma': 0.45259952757561295, 'step_size': 3, 'virtual_batch_size': 256, 'optimizer_type': 'adamw', 'p': 0.4973689952383496}. Best is trial 1 with value: 397.5173647525249.\n",
            " 10%|█         | 2/20 [11:14<1:39:29, 331.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 70 with best_epoch = 68 and best_valid_mae = 2.65379\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "[I 2025-09-11 17:06:32,434] Trial 2 finished with value: 8937463.792807037 and parameters: {'n_d': 11, 'n_a': 212, 'n_steps': 5, 'gamma': 55.101654019855125, 'lambda_sparse': 0.03227692680595638, 'batch_size': 2048, 'mask_type': 'sparsemax', 'emb': 43, 'momentum': 0.8392892907630071, 'learning_rate': 35.49033169860902, 'weight_decay': 5.020979031468037e-06, 'scheduler_gamma': 0.7402610293854881, 'step_size': 8, 'virtual_batch_size': 1024, 'optimizer_type': 'adam', 'p': 0.5865207668004241}. Best is trial 1 with value: 397.5173647525249.\n",
            " 15%|█▌        | 3/20 [19:47<1:57:26, 414.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 70 with best_epoch = 55 and best_valid_mae = 0.04464\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "[I 2025-09-11 17:17:22,535] Trial 3 finished with value: 21.272145899146267 and parameters: {'n_d': 33, 'n_a': 213, 'n_steps': 9, 'gamma': 22.661937972843344, 'lambda_sparse': 1.259065807183134e-11, 'batch_size': 4096, 'mask_type': 'sparsemax', 'emb': 53, 'momentum': 0.15481533174484316, 'learning_rate': 19.340482478112012, 'weight_decay': 1.3944588429462397e-06, 'scheduler_gamma': 0.14703943661575894, 'step_size': 15, 'virtual_batch_size': 1024, 'optimizer_type': 'rmsprop', 'p': 0.8164508777869709}. Best is trial 3 with value: 21.272145899146267.\n",
            " 20%|██        | 4/20 [30:37<2:15:20, 507.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 60 with best_epoch = 20 and best_valid_mae = 10.43009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "[I 2025-09-11 17:34:45,135] Trial 4 finished with value: 2038302480.8580682 and parameters: {'n_d': 228, 'n_a': 233, 'n_steps': 8, 'gamma': 65.91206170828102, 'lambda_sparse': 2.0913389525723742, 'batch_size': 1024, 'mask_type': 'sparsemax', 'emb': 66, 'momentum': 0.03995179252079938, 'learning_rate': 4.20500152511888, 'weight_decay': 0.0009783991297982494, 'scheduler_gamma': 0.8593686976204018, 'step_size': 9, 'virtual_batch_size': 1024, 'optimizer_type': 'rmsprop', 'p': 0.9676167346536377}. Best is trial 3 with value: 21.272145899146267.\n",
            " 25%|██▌       | 5/20 [48:00<2:55:07, 700.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 70 with best_epoch = 65 and best_valid_mae = 0.00131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "[I 2025-09-11 17:44:12,107] Trial 5 finished with value: 0.134551864874649 and parameters: {'n_d': 136, 'n_a': 82, 'n_steps': 2, 'gamma': 61.58542185849765, 'lambda_sparse': 0.17026512966010113, 'batch_size': 1024, 'mask_type': 'entmax', 'emb': 45, 'momentum': 0.7515157945275717, 'learning_rate': 0.08779748186052796, 'weight_decay': 1.706501547402328e-05, 'scheduler_gamma': 0.5575353100830288, 'step_size': 10, 'virtual_batch_size': 512, 'optimizer_type': 'rmsprop', 'p': 0.012559852015965677}. Best is trial 5 with value: 0.134551864874649.\n",
            " 30%|███       | 6/20 [57:27<2:32:51, 655.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 70 with best_epoch = 64 and best_valid_mae = 0.00101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "[I 2025-09-11 18:00:00,832] Trial 6 finished with value: 0.06870629379665665 and parameters: {'n_d': 75, 'n_a': 253, 'n_steps': 10, 'gamma': 77.13726204291119, 'lambda_sparse': 2.033600968988791e-11, 'batch_size': 2048, 'mask_type': 'sparsemax', 'emb': 41, 'momentum': 0.5158252222355557, 'learning_rate': 0.0021040765227438576, 'weight_decay': 8.376844290145493e-06, 'scheduler_gamma': 0.796171403854875, 'step_size': 3, 'virtual_batch_size': 512, 'optimizer_type': 'rmsprop', 'p': 0.14330200935617907}. Best is trial 6 with value: 0.06870629379665665.\n",
            " 35%|███▌      | 7/20 [1:13:15<2:42:44, 751.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 70 with best_epoch = 65 and best_valid_mae = 0.00088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "[I 2025-09-11 18:10:37,703] Trial 7 finished with value: 0.11391084363622009 and parameters: {'n_d': 187, 'n_a': 15, 'n_steps': 8, 'gamma': 8.045391733621763, 'lambda_sparse': 0.0014652361029021176, 'batch_size': 4096, 'mask_type': 'entmax', 'emb': 63, 'momentum': 0.7147496576978352, 'learning_rate': 0.001956198118192057, 'weight_decay': 0.00027227644241387247, 'scheduler_gamma': 0.6063949475537203, 'step_size': 8, 'virtual_batch_size': 512, 'optimizer_type': 'rmsprop', 'p': 0.5992749429156566}. Best is trial 6 with value: 0.06870629379665665.\n",
            " 40%|████      | 8/20 [1:23:52<2:22:56, 714.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 70 with best_epoch = 35 and best_valid_mae = 0.51037\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "[I 2025-09-11 18:17:00,030] Trial 8 finished with value: 18479.558845161224 and parameters: {'n_d': 232, 'n_a': 89, 'n_steps': 1, 'gamma': 82.88664389580218, 'lambda_sparse': 51.53592279068219, 'batch_size': 2048, 'mask_type': 'entmax', 'emb': 56, 'momentum': 0.5684219031761095, 'learning_rate': 1.6779101090622274e-07, 'weight_decay': 6.546390997948744e-05, 'scheduler_gamma': 0.19614930939489625, 'step_size': 3, 'virtual_batch_size': 256, 'optimizer_type': 'adamw', 'p': 0.6885503429942427}. Best is trial 6 with value: 0.06870629379665665.\n",
            " 45%|████▌     | 9/20 [1:30:15<1:51:58, 610.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 70 with best_epoch = 30 and best_valid_mae = 0.00935\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "[I 2025-09-11 18:39:39,872] Trial 9 finished with value: 0.9965631040242399 and parameters: {'n_d': 76, 'n_a': 235, 'n_steps': 10, 'gamma': 9.078302336833032, 'lambda_sparse': 4.926014143503172e-11, 'batch_size': 1024, 'mask_type': 'sparsemax', 'emb': 70, 'momentum': 0.7322070212236788, 'learning_rate': 0.5225686305599698, 'weight_decay': 1.2031455595255024e-06, 'scheduler_gamma': 0.2810409578500739, 'step_size': 10, 'virtual_batch_size': 256, 'optimizer_type': 'adam', 'p': 0.10011617751052164}. Best is trial 6 with value: 0.06870629379665665.\n",
            " 50%|█████     | 10/20 [1:52:54<2:20:20, 842.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 67 with best_epoch = 27 and best_valid_mae = 1.33207\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "[I 2025-09-11 18:49:36,370] Trial 10 finished with value: 65882.79495530597 and parameters: {'n_d': 136, 'n_a': 163, 'n_steps': 6, 'gamma': 99.06204466001327, 'lambda_sparse': 6.289284799679228e-07, 'batch_size': 2048, 'mask_type': 'entmax', 'emb': 16, 'momentum': 0.28899747874857185, 'learning_rate': 8.177295396632677e-05, 'weight_decay': 7.567873317090115e-05, 'scheduler_gamma': 0.9515071570336336, 'step_size': 20, 'virtual_batch_size': 512, 'optimizer_type': 'sgd', 'p': 0.2945776344171226}. Best is trial 6 with value: 0.06870629379665665.\n",
            " 55%|█████▌    | 11/20 [2:02:51<1:55:02, 766.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 65 with best_epoch = 25 and best_valid_mae = 0.0088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "[I 2025-09-11 18:59:53,969] Trial 11 finished with value: 5.061862852667332 and parameters: {'n_d': 182, 'n_a': 32, 'n_steps': 7, 'gamma': 74.36859608964107, 'lambda_sparse': 1.2071815377648762e-05, 'batch_size': 2048, 'mask_type': 'entmax', 'emb': 19, 'momentum': 0.4060833233938917, 'learning_rate': 0.0029615351740656876, 'weight_decay': 0.0004175865674848439, 'scheduler_gamma': 0.716396624421976, 'step_size': 1, 'virtual_batch_size': 512, 'optimizer_type': 'rmsprop', 'p': 0.3290608715343052}. Best is trial 6 with value: 0.06870629379665665.\n",
            " 60%|██████    | 12/20 [2:13:08<1:36:11, 721.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 70 with best_epoch = 66 and best_valid_mae = 0.00092\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "[I 2025-09-11 19:10:50,141] Trial 12 finished with value: 0.08381290315922103 and parameters: {'n_d': 181, 'n_a': 15, 'n_steps': 10, 'gamma': 36.95293584882349, 'lambda_sparse': 8.092930185600877e-06, 'batch_size': 4096, 'mask_type': 'entmax', 'emb': 3, 'momentum': 0.5061074210679336, 'learning_rate': 0.004340153535951104, 'weight_decay': 0.000174010636087244, 'scheduler_gamma': 0.7011569903519305, 'step_size': 6, 'virtual_batch_size': 512, 'optimizer_type': 'rmsprop', 'p': 0.32044062305527354}. Best is trial 6 with value: 0.06870629379665665.\n",
            " 65%|██████▌   | 13/20 [2:24:05<1:21:51, 701.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 70 with best_epoch = 66 and best_valid_mae = 1.63339\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "[I 2025-09-11 19:22:15,045] Trial 13 finished with value: 78767.19615227438 and parameters: {'n_d': 88, 'n_a': 158, 'n_steps': 10, 'gamma': 40.496803032318276, 'lambda_sparse': 3.297016798517748e-08, 'batch_size': 4096, 'mask_type': 'entmax', 'emb': 3, 'momentum': 0.43868269695730666, 'learning_rate': 0.04258036069090253, 'weight_decay': 4.5878679451239165e-06, 'scheduler_gamma': 0.7847658533588665, 'step_size': 5, 'virtual_batch_size': 512, 'optimizer_type': 'sgd', 'p': 0.28381063447797383}. Best is trial 6 with value: 0.06870629379665665.\n",
            " 70%|███████   | 14/20 [2:35:30<1:09:39, 696.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 70 with best_epoch = 64 and best_valid_mae = 0.65061\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "[I 2025-09-11 19:35:51,734] Trial 14 finished with value: 14247.788698630791 and parameters: {'n_d': 174, 'n_a': 82, 'n_steps': 10, 'gamma': 91.45134159493354, 'lambda_sparse': 1.9567252134421635e-12, 'batch_size': 2048, 'mask_type': 'sparsemax', 'emb': 24, 'momentum': 0.5232997335870401, 'learning_rate': 4.3726303006942516e-05, 'weight_decay': 0.0001095203367008332, 'scheduler_gamma': 0.9107921456633107, 'step_size': 5, 'virtual_batch_size': 512, 'optimizer_type': 'rmsprop', 'p': 0.18239478794419606}. Best is trial 6 with value: 0.06870629379665665.\n",
            " 75%|███████▌  | 15/20 [2:49:06<1:01:04, 732.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 70 with best_epoch = 61 and best_valid_mae = 0.23712\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "[I 2025-09-11 19:45:03,470] Trial 15 finished with value: 2434.635130893283 and parameters: {'n_d': 51, 'n_a': 51, 'n_steps': 8, 'gamma': 44.57343610089854, 'lambda_sparse': 2.6535059120522886e-05, 'batch_size': 4096, 'mask_type': 'entmax', 'emb': 27, 'momentum': 0.3511888916220908, 'learning_rate': 0.0005577605403484627, 'weight_decay': 5.694659780887386e-06, 'scheduler_gamma': 0.6511974069034726, 'step_size': 5, 'virtual_batch_size': 512, 'optimizer_type': 'rmsprop', 'p': 0.42591820937898167}. Best is trial 6 with value: 0.06870629379665665.\n",
            " 80%|████████  | 16/20 [2:58:18<45:13, 678.31s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 63 with best_epoch = 23 and best_valid_mae = 1.51242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "[I 2025-09-11 19:53:56,548] Trial 16 finished with value: 76782.7224414257 and parameters: {'n_d': 120, 'n_a': 256, 'n_steps': 5, 'gamma': 29.08080092164517, 'lambda_sparse': 1.92641075413058e-09, 'batch_size': 2048, 'mask_type': 'sparsemax', 'emb': 3, 'momentum': 0.5339670650879443, 'learning_rate': 2.363630398285578e-06, 'weight_decay': 0.00019596161627699672, 'scheduler_gamma': 0.39844984304796044, 'step_size': 1, 'virtual_batch_size': 512, 'optimizer_type': 'rmsprop', 'p': 0.21074436255913287}. Best is trial 6 with value: 0.06870629379665665.\n",
            " 85%|████████▌ | 17/20 [3:07:11<31:43, 634.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 70 with best_epoch = 68 and best_valid_mae = 0.00105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "[I 2025-09-11 20:08:01,920] Trial 17 finished with value: 0.08798476236308415 and parameters: {'n_d': 255, 'n_a': 145, 'n_steps': 9, 'gamma': 74.16295962322074, 'lambda_sparse': 3.3461740707298145e-07, 'batch_size': 4096, 'mask_type': 'entmax', 'emb': 30, 'momentum': 0.2784623773288536, 'learning_rate': 0.037065706773623724, 'weight_decay': 3.874126372062282e-05, 'scheduler_gamma': 0.8211040213378249, 'step_size': 6, 'virtual_batch_size': 512, 'optimizer_type': 'adamw', 'p': 0.4114862236901526}. Best is trial 6 with value: 0.06870629379665665.\n",
            " 90%|█████████ | 18/20 [3:21:16<23:15, 697.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 70 with best_epoch = 68 and best_valid_mae = 1.30382\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "[I 2025-09-11 20:16:35,156] Trial 18 finished with value: 68252.9669945618 and parameters: {'n_d': 115, 'n_a': 190, 'n_steps': 4, 'gamma': 50.15152115579969, 'lambda_sparse': 1.0986303076644433e-10, 'batch_size': 2048, 'mask_type': 'entmax', 'emb': 11, 'momentum': 0.5829240123781667, 'learning_rate': 0.00029427083977075723, 'weight_decay': 0.0008497467196354667, 'scheduler_gamma': 0.6732031678599523, 'step_size': 13, 'virtual_batch_size': 512, 'optimizer_type': 'sgd', 'p': 0.18071255537172864}. Best is trial 6 with value: 0.06870629379665665.\n",
            " 95%|█████████▌| 19/20 [3:29:50<10:42, 642.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop training because you reached max_epochs = 70 with best_epoch = 69 and best_valid_mae = 0.23935\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "[I 2025-09-11 20:34:16,992] Trial 19 finished with value: 3642.428342554558 and parameters: {'n_d': 158, 'n_a': 112, 'n_steps': 7, 'gamma': 68.67530934132684, 'lambda_sparse': 3.4898664758216643e-09, 'batch_size': 1024, 'mask_type': 'sparsemax', 'emb': 36, 'momentum': 0.8941608323606214, 'learning_rate': 7.280527799058792e-06, 'weight_decay': 9.329815847905553e-06, 'scheduler_gamma': 0.9661013857475702, 'step_size': 17, 'virtual_batch_size': 256, 'optimizer_type': 'rmsprop', 'p': 0.37840918129724105}. Best is trial 6 with value: 0.06870629379665665.\n",
            "100%|██████████| 20/20 [3:47:31<00:00, 682.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters for regression:  {'n_d': 75, 'n_a': 253, 'n_steps': 10, 'gamma': 77.13726204291119, 'lambda_sparse': 2.033600968988791e-11, 'batch_size': 2048, 'mask_type': 'sparsemax', 'emb': 41, 'momentum': 0.5158252222355557, 'learning_rate': 0.0021040765227438576, 'weight_decay': 8.376844290145493e-06, 'scheduler_gamma': 0.796171403854875, 'step_size': 3, 'virtual_batch_size': 512, 'optimizer_type': 'rmsprop', 'p': 0.14330200935617907}\n",
            "Best mae:  0.06870629379665665\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = par\n",
        "\n",
        "n_d = best_params['n_d']; n_a = best_params['n_a']; n_steps = best_params['n_steps']\n",
        "gamma = best_params['gamma']; lambda_sparse = best_params['lambda_sparse']\n",
        "mask_type = best_params['mask_type']; batch_size = best_params['batch_size']\n",
        "emb = best_params['emb']; p = best_params['p']\n",
        "momentum = best_params['momentum']; learning_rate = best_params['learning_rate']\n",
        "weight_decay = best_params['weight_decay']\n",
        "scheduler_gamma = best_params['scheduler_gamma']; step_size = best_params['step_size']\n",
        "virtual_batch_size = best_params['virtual_batch_size']; optimizer_type = best_params['optimizer_type']\n",
        "\n",
        "# Optimizer config (misma lógica)\n",
        "optimizer_fn, optimizer_params = optimizer_fn, optimizer_params = build_optimizer(optimizer_type, learning_rate, momentum, weight_decay)\n",
        "\n",
        "\n",
        "# Aumento y categóricas\n",
        "aug = RegressionSMOTE(p=p)\n",
        "cat_idxs = [i for i, f in enumerate(features) if f in CATEGORICAL_COLUMNS]\n",
        "cat_dims = [categorical_dims[f] for f in features if f in CATEGORICAL_COLUMNS]\n",
        "cat_emb_dim = [min(emb, (dim + 1)//2) for dim in cat_dims]\n",
        "\n",
        "clf = CustomTabNetRegressor(\n",
        "    cat_dims=cat_dims, cat_emb_dim=cat_emb_dim, cat_idxs=cat_idxs,\n",
        "    n_d=n_d, n_a=n_a, n_steps=n_steps, gamma=gamma, lambda_sparse=lambda_sparse,\n",
        "    mask_type=mask_type, optimizer_fn=optimizer_fn, optimizer_params=optimizer_params,\n",
        "    scheduler_params={\"gamma\": scheduler_gamma, \"step_size\": step_size},\n",
        "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "    momentum=momentum, verbose=True\n",
        ")\n",
        "\n",
        "clf.fit(\n",
        "    X_train=X_train, y_train=y_train[:,0:1],\n",
        "    eval_set=[(X_train, y_train[:,0:1]), (X_valid, y_valid[:,0:1])],\n",
        "    eval_name=['train', 'valid'], eval_metric=['mae'],\n",
        "    loss_fn=my_r2_score_fn,\n",
        "    max_epochs=200, patience=70,\n",
        "    batch_size=batch_size, virtual_batch_size=virtual_batch_size,\n",
        "    num_workers=1, drop_last=False, augmentations=aug,\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aKnXo3CNUsM7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7bf3704-0992-42d1-82ad-767b391dff85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 44989.84148| train_mae: 3.53835 | valid_mae: 3.52597 |  0:00:12s\n",
            "epoch 1  | loss: 1896.84927| train_mae: 0.59406 | valid_mae: 0.59604 |  0:00:25s\n",
            "epoch 2  | loss: 358.01071| train_mae: 0.25387 | valid_mae: 0.25235 |  0:00:37s\n",
            "epoch 3  | loss: 122.70116| train_mae: 0.06537 | valid_mae: 0.06684 |  0:00:50s\n",
            "epoch 4  | loss: 70.43245| train_mae: 0.08271 | valid_mae: 0.0832  |  0:01:02s\n",
            "epoch 5  | loss: 28.69788| train_mae: 0.05096 | valid_mae: 0.05115 |  0:01:15s\n",
            "epoch 6  | loss: 19.93663| train_mae: 0.02972 | valid_mae: 0.03072 |  0:01:28s\n",
            "epoch 7  | loss: 8.29522 | train_mae: 0.03932 | valid_mae: 0.03929 |  0:01:41s\n",
            "epoch 8  | loss: 5.63204 | train_mae: 0.00843 | valid_mae: 0.0087  |  0:01:54s\n",
            "epoch 9  | loss: 1.67899 | train_mae: 0.00554 | valid_mae: 0.00571 |  0:02:06s\n",
            "epoch 10 | loss: 2.22248 | train_mae: 0.01596 | valid_mae: 0.0163  |  0:02:19s\n",
            "epoch 11 | loss: 2.1938  | train_mae: 0.00835 | valid_mae: 0.00841 |  0:02:32s\n",
            "epoch 12 | loss: 1.35881 | train_mae: 0.00871 | valid_mae: 0.00895 |  0:02:45s\n",
            "epoch 13 | loss: 1.57158 | train_mae: 0.00438 | valid_mae: 0.00448 |  0:02:58s\n",
            "epoch 14 | loss: 1.04744 | train_mae: 0.00388 | valid_mae: 0.00402 |  0:03:11s\n",
            "epoch 15 | loss: 0.72382 | train_mae: 0.00794 | valid_mae: 0.00813 |  0:03:24s\n",
            "epoch 16 | loss: 0.73128 | train_mae: 0.00832 | valid_mae: 0.00843 |  0:03:36s\n",
            "epoch 17 | loss: 0.9008  | train_mae: 0.00445 | valid_mae: 0.00459 |  0:03:49s\n",
            "epoch 18 | loss: 0.57346 | train_mae: 0.00658 | valid_mae: 0.00707 |  0:04:03s\n",
            "epoch 19 | loss: 0.55565 | train_mae: 0.00319 | valid_mae: 0.00333 |  0:04:16s\n",
            "epoch 20 | loss: 0.52661 | train_mae: 0.00423 | valid_mae: 0.00508 |  0:04:31s\n",
            "epoch 21 | loss: 0.61004 | train_mae: 0.00374 | valid_mae: 0.00413 |  0:04:45s\n",
            "epoch 22 | loss: 0.41246 | train_mae: 0.00344 | valid_mae: 0.00385 |  0:04:58s\n",
            "epoch 23 | loss: 0.44258 | train_mae: 0.00366 | valid_mae: 0.00375 |  0:05:12s\n",
            "epoch 24 | loss: 0.38585 | train_mae: 0.00435 | valid_mae: 0.0048  |  0:05:25s\n",
            "epoch 25 | loss: 0.30447 | train_mae: 0.00317 | valid_mae: 0.00372 |  0:05:37s\n",
            "epoch 26 | loss: 0.31352 | train_mae: 0.00338 | valid_mae: 0.0039  |  0:05:50s\n",
            "epoch 27 | loss: 0.32187 | train_mae: 0.00399 | valid_mae: 0.00419 |  0:06:03s\n",
            "epoch 28 | loss: 0.30679 | train_mae: 0.00268 | valid_mae: 0.0032  |  0:06:16s\n",
            "epoch 29 | loss: 0.29063 | train_mae: 0.00261 | valid_mae: 0.00326 |  0:06:29s\n",
            "epoch 30 | loss: 0.34241 | train_mae: 0.00246 | valid_mae: 0.00292 |  0:06:42s\n",
            "epoch 31 | loss: 0.26111 | train_mae: 0.0027  | valid_mae: 0.00316 |  0:06:55s\n",
            "epoch 32 | loss: 0.25387 | train_mae: 0.00217 | valid_mae: 0.0023  |  0:07:08s\n",
            "epoch 33 | loss: 0.21684 | train_mae: 0.00233 | valid_mae: 0.00253 |  0:07:21s\n",
            "epoch 34 | loss: 0.22391 | train_mae: 0.00241 | valid_mae: 0.00258 |  0:07:34s\n",
            "epoch 35 | loss: 0.21656 | train_mae: 0.00249 | valid_mae: 0.00255 |  0:07:46s\n",
            "epoch 36 | loss: 0.21768 | train_mae: 0.00201 | valid_mae: 0.00204 |  0:07:59s\n",
            "epoch 37 | loss: 0.19239 | train_mae: 0.00201 | valid_mae: 0.00206 |  0:08:12s\n",
            "epoch 38 | loss: 0.22268 | train_mae: 0.00239 | valid_mae: 0.00252 |  0:08:24s\n",
            "epoch 39 | loss: 0.18998 | train_mae: 0.00218 | valid_mae: 0.00226 |  0:08:37s\n",
            "epoch 40 | loss: 0.185   | train_mae: 0.00215 | valid_mae: 0.00224 |  0:08:49s\n",
            "epoch 41 | loss: 0.17911 | train_mae: 0.00223 | valid_mae: 0.00237 |  0:09:02s\n",
            "epoch 42 | loss: 0.18692 | train_mae: 0.00226 | valid_mae: 0.00239 |  0:09:15s\n",
            "epoch 43 | loss: 0.16693 | train_mae: 0.00191 | valid_mae: 0.00207 |  0:09:28s\n",
            "epoch 44 | loss: 0.168   | train_mae: 0.00178 | valid_mae: 0.0019  |  0:09:40s\n",
            "epoch 45 | loss: 0.16394 | train_mae: 0.00211 | valid_mae: 0.00225 |  0:09:53s\n",
            "epoch 46 | loss: 0.16483 | train_mae: 0.0019  | valid_mae: 0.00198 |  0:10:07s\n",
            "epoch 47 | loss: 0.15598 | train_mae: 0.00203 | valid_mae: 0.00217 |  0:10:21s\n",
            "epoch 48 | loss: 0.16403 | train_mae: 0.00181 | valid_mae: 0.00196 |  0:10:36s\n",
            "epoch 49 | loss: 0.15373 | train_mae: 0.00192 | valid_mae: 0.00209 |  0:10:49s\n",
            "epoch 50 | loss: 0.15524 | train_mae: 0.00176 | valid_mae: 0.00183 |  0:11:02s\n",
            "epoch 51 | loss: 0.15285 | train_mae: 0.00199 | valid_mae: 0.00207 |  0:11:15s\n",
            "epoch 52 | loss: 0.1552  | train_mae: 0.00177 | valid_mae: 0.00194 |  0:11:27s\n",
            "epoch 53 | loss: 0.15956 | train_mae: 0.00184 | valid_mae: 0.00196 |  0:11:40s\n",
            "epoch 54 | loss: 0.14544 | train_mae: 0.00171 | valid_mae: 0.00184 |  0:11:52s\n",
            "epoch 55 | loss: 0.32945 | train_mae: 0.00483 | valid_mae: 0.0053  |  0:12:05s\n",
            "epoch 56 | loss: 0.4421  | train_mae: 0.00429 | valid_mae: 0.00467 |  0:12:18s\n",
            "epoch 57 | loss: 0.38138 | train_mae: 0.0031  | valid_mae: 0.00332 |  0:12:30s\n",
            "epoch 58 | loss: 0.36552 | train_mae: 0.00294 | valid_mae: 0.00322 |  0:12:44s\n",
            "epoch 59 | loss: 0.33698 | train_mae: 0.00284 | valid_mae: 0.00319 |  0:12:56s\n",
            "epoch 60 | loss: 0.31086 | train_mae: 0.00272 | valid_mae: 0.00297 |  0:13:08s\n",
            "epoch 61 | loss: 0.28371 | train_mae: 0.0026  | valid_mae: 0.00274 |  0:13:21s\n",
            "epoch 62 | loss: 0.2654  | train_mae: 0.00263 | valid_mae: 0.00269 |  0:13:33s\n",
            "epoch 63 | loss: 0.24983 | train_mae: 0.00242 | valid_mae: 0.00252 |  0:13:45s\n",
            "epoch 64 | loss: 0.23877 | train_mae: 0.0023  | valid_mae: 0.00246 |  0:13:58s\n",
            "epoch 65 | loss: 0.24555 | train_mae: 0.00233 | valid_mae: 0.00243 |  0:14:11s\n",
            "epoch 66 | loss: 0.23506 | train_mae: 0.00232 | valid_mae: 0.00248 |  0:14:23s\n",
            "epoch 67 | loss: 0.22302 | train_mae: 0.00228 | valid_mae: 0.00238 |  0:14:36s\n",
            "epoch 68 | loss: 0.23407 | train_mae: 0.00233 | valid_mae: 0.00242 |  0:14:49s\n",
            "epoch 69 | loss: 0.22958 | train_mae: 0.00208 | valid_mae: 0.00219 |  0:15:02s\n",
            "epoch 70 | loss: 0.22842 | train_mae: 0.00225 | valid_mae: 0.00237 |  0:15:15s\n",
            "epoch 71 | loss: 0.22221 | train_mae: 0.00232 | valid_mae: 0.00244 |  0:15:28s\n",
            "epoch 72 | loss: 0.22039 | train_mae: 0.00223 | valid_mae: 0.00234 |  0:15:40s\n",
            "epoch 73 | loss: 0.21762 | train_mae: 0.00204 | valid_mae: 0.00214 |  0:15:54s\n",
            "epoch 74 | loss: 0.22651 | train_mae: 0.00214 | valid_mae: 0.00224 |  0:16:08s\n",
            "epoch 75 | loss: 0.20969 | train_mae: 0.00218 | valid_mae: 0.00227 |  0:16:23s\n",
            "epoch 76 | loss: 0.20957 | train_mae: 0.0022  | valid_mae: 0.00245 |  0:16:36s\n",
            "epoch 77 | loss: 0.1993  | train_mae: 0.00196 | valid_mae: 0.00209 |  0:16:49s\n",
            "epoch 78 | loss: 0.21334 | train_mae: 0.00207 | valid_mae: 0.00218 |  0:17:02s\n",
            "epoch 79 | loss: 0.20065 | train_mae: 0.00207 | valid_mae: 0.00227 |  0:17:14s\n",
            "epoch 80 | loss: 0.2022  | train_mae: 0.002   | valid_mae: 0.00215 |  0:17:27s\n",
            "epoch 81 | loss: 0.20229 | train_mae: 0.00205 | valid_mae: 0.00219 |  0:17:40s\n",
            "epoch 82 | loss: 0.20205 | train_mae: 0.00196 | valid_mae: 0.00215 |  0:17:53s\n",
            "epoch 83 | loss: 0.18942 | train_mae: 0.00213 | valid_mae: 0.00225 |  0:18:06s\n",
            "epoch 84 | loss: 0.19837 | train_mae: 0.00196 | valid_mae: 0.00209 |  0:18:19s\n",
            "epoch 85 | loss: 0.20688 | train_mae: 0.00194 | valid_mae: 0.00203 |  0:18:32s\n",
            "epoch 86 | loss: 0.20077 | train_mae: 0.00243 | valid_mae: 0.0025  |  0:18:44s\n",
            "epoch 87 | loss: 0.1864  | train_mae: 0.00197 | valid_mae: 0.00208 |  0:18:57s\n",
            "epoch 88 | loss: 0.20695 | train_mae: 0.00198 | valid_mae: 0.00213 |  0:19:10s\n",
            "epoch 89 | loss: 0.19551 | train_mae: 0.00202 | valid_mae: 0.0021  |  0:19:22s\n",
            "epoch 90 | loss: 0.19501 | train_mae: 0.00199 | valid_mae: 0.0021  |  0:19:35s\n",
            "epoch 91 | loss: 0.19635 | train_mae: 0.00192 | valid_mae: 0.00204 |  0:19:47s\n",
            "epoch 92 | loss: 0.20398 | train_mae: 0.00195 | valid_mae: 0.00208 |  0:20:00s\n",
            "epoch 93 | loss: 0.19234 | train_mae: 0.00199 | valid_mae: 0.00211 |  0:20:13s\n",
            "epoch 94 | loss: 0.19341 | train_mae: 0.00192 | valid_mae: 0.00202 |  0:20:25s\n",
            "epoch 95 | loss: 0.19381 | train_mae: 0.00195 | valid_mae: 0.00205 |  0:20:38s\n",
            "epoch 96 | loss: 0.18446 | train_mae: 0.00192 | valid_mae: 0.00208 |  0:20:50s\n",
            "epoch 97 | loss: 0.19475 | train_mae: 0.002   | valid_mae: 0.00212 |  0:21:02s\n",
            "epoch 98 | loss: 0.19308 | train_mae: 0.00194 | valid_mae: 0.00206 |  0:21:15s\n",
            "epoch 99 | loss: 0.18994 | train_mae: 0.00194 | valid_mae: 0.00207 |  0:21:27s\n",
            "epoch 100| loss: 0.17932 | train_mae: 0.00205 | valid_mae: 0.00216 |  0:21:40s\n",
            "epoch 101| loss: 0.19456 | train_mae: 0.00198 | valid_mae: 0.00208 |  0:21:54s\n",
            "epoch 102| loss: 0.18078 | train_mae: 0.00207 | valid_mae: 0.00218 |  0:22:08s\n",
            "epoch 103| loss: 0.17903 | train_mae: 0.00191 | valid_mae: 0.00204 |  0:22:21s\n",
            "epoch 104| loss: 0.18445 | train_mae: 0.00195 | valid_mae: 0.00204 |  0:22:34s\n",
            "epoch 105| loss: 0.18332 | train_mae: 0.00187 | valid_mae: 0.00206 |  0:22:46s\n",
            "epoch 106| loss: 0.18667 | train_mae: 0.00192 | valid_mae: 0.00207 |  0:22:59s\n",
            "epoch 107| loss: 0.18307 | train_mae: 0.00198 | valid_mae: 0.00213 |  0:23:11s\n",
            "epoch 108| loss: 0.19926 | train_mae: 0.00181 | valid_mae: 0.00199 |  0:23:24s\n",
            "epoch 109| loss: 0.19045 | train_mae: 0.00209 | valid_mae: 0.00227 |  0:23:37s\n",
            "epoch 110| loss: 0.19519 | train_mae: 0.00191 | valid_mae: 0.00208 |  0:23:50s\n",
            "epoch 111| loss: 0.18146 | train_mae: 0.002   | valid_mae: 0.00219 |  0:24:03s\n",
            "epoch 112| loss: 0.1824  | train_mae: 0.00191 | valid_mae: 0.00212 |  0:24:16s\n",
            "epoch 113| loss: 0.1961  | train_mae: 0.00185 | valid_mae: 0.002   |  0:24:28s\n",
            "epoch 114| loss: 0.19372 | train_mae: 0.00193 | valid_mae: 0.0021  |  0:24:41s\n",
            "epoch 115| loss: 0.19028 | train_mae: 0.00202 | valid_mae: 0.00217 |  0:24:54s\n",
            "epoch 116| loss: 0.18454 | train_mae: 0.00191 | valid_mae: 0.00206 |  0:25:07s\n",
            "epoch 117| loss: 0.19346 | train_mae: 0.00204 | valid_mae: 0.00224 |  0:25:20s\n",
            "epoch 118| loss: 0.18727 | train_mae: 0.00195 | valid_mae: 0.0021  |  0:25:32s\n",
            "epoch 119| loss: 0.19797 | train_mae: 0.00185 | valid_mae: 0.00201 |  0:25:45s\n",
            "epoch 120| loss: 0.18441 | train_mae: 0.00192 | valid_mae: 0.00207 |  0:25:57s\n",
            "\n",
            "Early stopping occurred at epoch 120 with best_epoch = 50 and best_valid_mae = 0.00183\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=clf.predict(X_test)\n",
        "y_pred_ = scaler.inverse_transform(y_pred)\n",
        "y_test_ = scaler.inverse_transform(y_test)\n",
        "# Plot 2\n",
        "r2_1 = r2_score(y_test, y_pred)\n",
        "print(f\" Test  -> R2={r2_1:.4f}\")"
      ],
      "metadata": {
        "id": "9avV4zk8Wafg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "769291d0-76dd-4f79-e35a-aac09ed2801f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Test  -> R2=0.8577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(clf, \"/content/drive/MyDrive/Colab Notebooks/Diego/model.pth\")"
      ],
      "metadata": {
        "id": "FllT3OXdVjFB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}