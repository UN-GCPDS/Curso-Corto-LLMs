{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UN-GCPDS/Curso-Corto-LLMs/blob/main/3.%20Dashboard/Generaci%C3%B3n_de_M%C3%A1scara.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Logo UNAL CHEC](https://github.com/UN-GCPDS/curso_IA_CHEC/blob/main/logo_unal_chec.jpg?raw=1)\n",
        "\n",
        "# **Generación de Máscara**\n",
        "\n",
        "## **Descripción**\n",
        "\n",
        "Entrenamiento de modelo Tabnet bajo diversas condiciones.\n",
        "\n",
        "### **Profesor - Sesión 1:** Andrés Marino Álvarez Meza y Diego Armando Pérez Rosero"
      ],
      "metadata": {
        "id": "mFZXuItQprV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuPas77Kxpn6",
        "outputId": "856afd4f-fd80-49ac-8e8f-7c1feed7abf4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datos\n",
        "\n",
        "**TabNet para criticidad en redes de media tensión — Planteamiento y datos (Regresión)**\n",
        "\n",
        "Sea el conjunto de datos\n",
        "\n",
        "$$\n",
        "\\mathbf{X}\\in\\mathbb{R}^{N\\times M},\\qquad\n",
        "\\mathbf{y}\\in\\mathbb{R}^{N}.\n",
        "$$\n",
        "\n",
        "Cada fila de $\\mathbf{X}$ representa un evento o periodo entre 2019 y 2024 y contiene las características de los elementos asociados al equipo que operó. El vector $\\mathbf{y}$ almacena el valor continuo del indicador a modelar (SAIDI o SAIFI) para ese mismo evento/periodo.\n",
        "\n",
        "Definimos\n",
        "\n",
        "$$\n",
        "\\mathcal{F}:\\mathcal{X}\\subseteq\\mathbb{R}^{M}\\to\\mathbb{R},\\qquad\n",
        "\\hat{y}=\\mathcal{F}(\\mathbf{x})\n",
        "=\n",
        "\\bigl(\\,\\breve{f}_{L}\\circ \\breve{f}_{L-1}\\circ \\cdots \\circ \\breve{f}_{1}\\,\\bigr)(\\mathbf{x}),\n",
        "$$\n",
        "\n",
        "donde $\\breve{f}_{l}(\\cdot)$ denota el $l$-ésimo bloque del modelo ($l\\in\\{1,\\dots,L\\}$) y $\\circ$ es el operador de composición.\n",
        "\n",
        "En caso multisalida para $(\\text{SAIDI},\\text{SAIFI})$, se toma $\\mathcal{F}:\\mathbb{R}^{M}\\to\\mathbb{R}^{2}$ y $\\mathbf{y}\\in\\mathbb{R}^{N\\times 2}$.\n",
        "![Logo UNAL CHEC](https://raw.githubusercontent.com/Daprosero/Deep-Convolutional-Generative-Adversarial-Network/refs/heads/master/Mercados%20CHEC.png)"
      ],
      "metadata": {
        "id": "NHmuwRJcqrHN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9h3KDG32hY8a",
        "outputId": "a12cfd82-ab3c-420b-f2cb-11eb78a410be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openTSNE\n",
            "  Downloading openTSNE-1.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.12/dist-packages (from openTSNE) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.12/dist-packages (from openTSNE) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from openTSNE) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.20->openTSNE) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.20->openTSNE) (3.6.0)\n",
            "Downloading openTSNE-1.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/3.2 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/3.2 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openTSNE\n",
            "Successfully installed openTSNE-1.0.2\n",
            "Collecting pytorch-tabnet\n",
            "  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from pytorch-tabnet) (2.0.2)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.12/dist-packages (from pytorch-tabnet) (1.6.1)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.12/dist-packages (from pytorch-tabnet) (1.16.1)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.12/dist-packages (from pytorch-tabnet) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.12/dist-packages (from pytorch-tabnet) (4.67.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.16.5)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.43)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (3.19.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.3->pytorch-tabnet) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.3->pytorch-tabnet) (3.0.2)\n",
            "Downloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna, pytorch-tabnet\n",
            "Successfully installed colorlog-6.9.0 optuna-4.5.0 pytorch-tabnet-4.1.0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "#@title Librerías\n",
        "# Instalación de paquetes necesarios\n",
        "!pip install -q gdown\n",
        "!pip install openTSNE\n",
        "!pip install pytorch-tabnet optuna\n",
        "!pip install wget --quiet\n",
        "\n",
        "# Importación de librerías necesarias\n",
        "import optuna\n",
        "import warnings\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import softmax\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, QuantileTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor, TabNetClassifier\n",
        "from pytorch_tabnet.augmentations import RegressionSMOTE\n",
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import os\n",
        "from pathlib import Path\n",
        "import math\n",
        "import wget\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Supresión de warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"pandas\")\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Función auxiliar para etiquetas\n",
        "def get_labels(x: pd.Series) -> pd.Series:\n",
        "    labels, _ = pd.factorize(x)\n",
        "    return pd.Series(labels, name=x.name, index=x.index)\n",
        "\n",
        "# Definición de funciones personalizadas de pérdida\n",
        "def my_mse_loss_fn(y_pred, y_true):\n",
        "    mse_loss = (y_true - y_pred) ** 2\n",
        "    return torch.mean(mse_loss)\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_var_band(\n",
        "    df,\n",
        "    var_token,\n",
        "    row_index=0,\n",
        "    hours_back=24,\n",
        "    col_patterns=None,\n",
        "    display_name=None,\n",
        "    units=None,\n",
        "    event_label=\"evento reportado\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Grafica una variable climática en una franja de horas hacia atrás.\n",
        "\n",
        "    Parámetros\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Contiene columnas por hora para la variable elegida.\n",
        "        Ejemplos de nombres soportados automáticamente:\n",
        "        - 'h0-<var>', 'h1-<var>', ..., 'h24-<var>'\n",
        "        - '<var>_h0', '<var>_h1', ...\n",
        "        con separadores '_' o '-'.\n",
        "\n",
        "    var_token : str\n",
        "        Nombre base de la variable en los nombres de columna (p.ej. 'wind_gust_spd',\n",
        "        'air_temp', 'precip'). Debe coincidir con lo que aparece en las columnas.\n",
        "\n",
        "    row_index : int\n",
        "        Fila (evento) a graficar.\n",
        "\n",
        "    hours_back : int\n",
        "        Cuántas horas hacia atrás mostrar.\n",
        "\n",
        "    col_patterns : list[str] | None\n",
        "        Lista de regex opcionales para detectar columnas por hora.\n",
        "        Si None, se generan automáticamente a partir de var_token.\n",
        "\n",
        "    display_name : str | None\n",
        "        Etiqueta legible para el eje Y (p.ej. 'Ráfaga de viento').\n",
        "        Si None, se usa var_token.\n",
        "\n",
        "    units : str | None\n",
        "        Unidades para concatenar en la etiqueta Y (p.ej. 'm/s', '°C', 'mm').\n",
        "\n",
        "    event_label : str\n",
        "        Texto para la flecha en la hora 0.\n",
        "    \"\"\"\n",
        "    # --- 1) Preparar patrones de columnas ---\n",
        "    if col_patterns is None:\n",
        "        # Permitir '_' o '-' (o espacio) entre partes del var_token\n",
        "        parts = re.split(r'[_\\-\\s]+', var_token.strip())\n",
        "        # Construimos un regex que tolere '_' o '-' entre partes\n",
        "        # ej: 'wind[_-]?gust[_-]?spd'\n",
        "        var_regex = r'[_-]?'.join(map(re.escape, parts))\n",
        "\n",
        "        col_patterns = [\n",
        "            rf'^h(\\d{{1,2}})[-_]?{var_regex}$',   # h0-<var>  o  h0_<var>\n",
        "            rf'^{var_regex}[-_]?h(\\d{{1,2}})$',   # <var>-h0  o  <var>_h0\n",
        "        ]\n",
        "\n",
        "    # --- 2) Detectar columnas y mapear a hora ---\n",
        "    hour_to_col = {}\n",
        "    for c in df.columns:\n",
        "        for pat in col_patterns:\n",
        "            m = re.match(pat, str(c), flags=re.IGNORECASE)\n",
        "            if m:\n",
        "                h = int(m.group(1))\n",
        "                hour_to_col[h] = c\n",
        "                break\n",
        "\n",
        "    if not hour_to_col:\n",
        "        raise ValueError(\n",
        "            f\"No se encontraron columnas con horas para la variable '{var_token}'.\\n\"\n",
        "            f\"Prueba ajustando 'var_token' o pasando 'col_patterns' personalizados.\"\n",
        "        )\n",
        "\n",
        "    # --- 3) Construir serie horas [0..hours_back] si existen, orden ascendente ---\n",
        "    hours = [h for h in sorted(hour_to_col.keys()) if 0 <= h <= hours_back]\n",
        "    vals = np.array(\n",
        "        [pd.to_numeric(df.loc[df.index[row_index], hour_to_col[h]], errors='coerce') for h in hours],\n",
        "        dtype=float\n",
        "    )\n",
        "\n",
        "    # --- 4) Graficar ---\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "    # línea y puntos\n",
        "    ax.plot(hours, vals, marker='o')\n",
        "\n",
        "    # invertir eje X para que se vea 24 -> 0\n",
        "    ax.set_xlim(hours_back, 0)\n",
        "\n",
        "    # franja sombreada\n",
        "    ymin = np.nanmin(vals) if np.isfinite(np.nanmin(vals)) else 0.0\n",
        "    ymax = np.nanmax(vals) if np.isfinite(np.nanmax(vals)) else 1.0\n",
        "    pad  = 0.05 * (ymax - ymin if ymax > ymin else 1.0)\n",
        "    ax.set_ylim(ymin - pad, ymax + pad)\n",
        "    ax.axvspan(0, hours_back, alpha=0.15)\n",
        "\n",
        "    # flecha y etiqueta en hora 0\n",
        "    y0 = vals[hours.index(0)] if 0 in hours else np.nan\n",
        "    if not np.isfinite(y0):\n",
        "        y0 = np.nanmedian(vals) if np.isfinite(np.nanmedian(vals)) else (ymin + ymax) / 2.0\n",
        "\n",
        "    ax.annotate(\n",
        "        event_label,\n",
        "        xy=(0, y0),\n",
        "        xytext=(max(2, min(4, hours_back*0.15)), y0 + (ymax - y0)*0.15),\n",
        "        arrowprops=dict(arrowstyle=\"->\", lw=1),\n",
        "        ha='left', va='bottom'\n",
        "    )\n",
        "\n",
        "    # etiquetas\n",
        "    ylab = display_name if display_name else var_token\n",
        "    if units:\n",
        "        ylab = f\"{ylab} [{units}]\"\n",
        "    ax.set_xlabel(\"Horas antes del evento\")\n",
        "    ax.set_ylabel(ylab)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # ticks principales (24, 18, 12, 6, 0) si corresponde\n",
        "    xticks = [h for h in [hours_back, 18, 12, 6, 0] if 0 <= h <= hours_back]\n",
        "    ax.set_xticks(xticks)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# --- Ejemplo de uso:\n",
        "# plot_wind_gust_band(df=tu_dataframe, row_index=0, hours_back=24)\n",
        "\n",
        "def my_rmse_loss_fn(y_pred, y_true):\n",
        "    mse_loss = (y_true - y_pred) ** 2\n",
        "    mean_mse_loss = torch.mean(mse_loss)\n",
        "    rmse_loss = torch.sqrt(mean_mse_loss)\n",
        "    return rmse_loss\n",
        "\n",
        "def my_mae_loss_fn(y_pred, y_true):\n",
        "    mae_loss = torch.abs(y_true - y_pred)\n",
        "    return torch.mean(mae_loss)\n",
        "\n",
        "def my_mape_loss_fn(y_pred, y_true):\n",
        "    mape_loss = torch.abs((y_true - y_pred) / y_true) * 100\n",
        "    return torch.mean(mape_loss)\n",
        "\n",
        "def my_r2_score_fn(y_pred, y_true):\n",
        "    total_variance = torch.var(y_true, unbiased=False)\n",
        "    unexplained_variance = torch.mean((y_true - y_pred) ** 2)\n",
        "    r2_score = 1 - (unexplained_variance / total_variance)\n",
        "    return 1-r2_score\n",
        "\n",
        "# Etapa 0: imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "# ==== Librerías ====\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "import xgboost as xgb\n",
        "\n",
        "from cuml.ensemble import RandomForestRegressor as cuRF\n",
        "from cuml.metrics import r2_score as r2_gpu\n",
        "\n",
        "# Si quieres comparar con CPU para sanity-check:\n",
        "from sklearn.metrics import r2_score as r2_cpu\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "# ==== Utilidades ====\n",
        "def to_cpu(a):\n",
        "    \"\"\"Convierte CuPy -> NumPy si aplica.\"\"\"\n",
        "    try:\n",
        "        if isinstance(a, cp.ndarray):\n",
        "            return cp.asnumpy(a)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return a\n",
        "\n",
        "def metrics_gpu(y_true_cp, y_pred_cp):\n",
        "    \"\"\"MAE, RMSE, R2 calculados en GPU (CuPy).\"\"\"\n",
        "    y_true_cp = cp.asarray(y_true_cp)\n",
        "    y_pred_cp = cp.asarray(y_pred_cp)\n",
        "    mae  = float(cp.mean(cp.abs(y_true_cp - y_pred_cp)))\n",
        "    rmse = float(cp.sqrt(cp.mean((y_true_cp - y_pred_cp)**2)))\n",
        "    ssr  = float(cp.sum((y_true_cp - y_pred_cp)**2))\n",
        "    sst  = float(cp.sum((y_true_cp - cp.mean(y_true_cp))**2))\n",
        "    r2   = 1.0 - ssr / sst if sst > 0 else np.nan\n",
        "    return mae, rmse, r2\n",
        "\n",
        "def permutation_importance_rf_gpu(model, X_val_cp, y_val_cp, n_repeats=3, max_feats=None, random_state=42):\n",
        "    \"\"\"\n",
        "    Permutation importance en GPU para RF cuML.\n",
        "    Devuelve importancia por feature (drop medio de R2 en valid).\n",
        "    Si max_feats no es None, calcula solo para las primeras max_feats columnas (para acelerar).\n",
        "    \"\"\"\n",
        "    rs = cp.random.RandomState(random_state)\n",
        "    X_val_cp = cp.asarray(X_val_cp)\n",
        "    y_val_cp = cp.asarray(y_val_cp)\n",
        "\n",
        "    # R2 base\n",
        "    y_pred_base = model.predict(X_val_cp)\n",
        "    _, _, r2_base = metrics_gpu(y_val_cp, y_pred_base)\n",
        "\n",
        "    n, d = X_val_cp.shape\n",
        "    d_eval = d if max_feats is None else int(min(max_feats, d))\n",
        "    importances = cp.zeros(d, dtype=cp.float32)\n",
        "\n",
        "    for j in range(d_eval):\n",
        "        drops = []\n",
        "        for _ in range(n_repeats):\n",
        "            Xp = X_val_cp.copy()\n",
        "            idx = rs.permutation(n)\n",
        "            Xp[:, j] = Xp[idx, j]  # permutar solo la columna j\n",
        "            y_pred_p = model.predict(Xp)\n",
        "            _, _, r2_p = metrics_gpu(y_val_cp, y_pred_p)\n",
        "            drops.append(r2_base - r2_p)\n",
        "        importances[j] = cp.mean(cp.asarray(drops))\n",
        "\n",
        "    return importances  # CuPy array\n",
        "def regression_metrics(y_true, y_pred):\n",
        "    mae  = float(np.mean(np.abs(y_true - y_pred)))\n",
        "    rmse = float(np.sqrt(np.mean((y_true - y_pred)**2)))\n",
        "    ss_res = float(np.sum((y_true - y_pred)**2))\n",
        "    ss_tot = float(np.sum((y_true - np.mean(y_true))**2))\n",
        "    r2 = 1 - ss_res/ss_tot if ss_tot > 0 else np.nan\n",
        "    return mae, rmse, r2\n",
        "class CustomTabNetRegressor(TabNetRegressor):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(CustomTabNetRegressor, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, X):\n",
        "        output, M_loss = self.network(X)\n",
        "        output = torch.relu(output)\n",
        "        return output, M_loss\n",
        "\n",
        "    def predict(self, X):\n",
        "        device = next(self.network.parameters()).device\n",
        "        if not isinstance(X, torch.Tensor):\n",
        "            X = torch.tensor(X, dtype=torch.float32)\n",
        "        X = X.to(device)\n",
        "        with torch.no_grad():\n",
        "            output, _ = self.forward(X)\n",
        "        return output.cpu().numpy()\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.spatial import cKDTree\n",
        "from tqdm import tqdm\n",
        "from ast import literal_eval\n",
        "from pandas.api.types import is_numeric_dtype\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "def make_strat_labels(y_vals, n_bins=3, min_per_class=2):\n",
        "    \"\"\"\n",
        "    Genera etiquetas para estratificar a partir de un objetivo continuo.\n",
        "    Reduce bins si no hay suficientes muestras por clase.\n",
        "    \"\"\"\n",
        "    y1d = y_vals.reshape(-1)\n",
        "    for bins in range(n_bins, 1, -1):\n",
        "        pct = np.linspace(0, 100, bins + 1)[1:-1]\n",
        "        cuts = np.percentile(y1d, pct)\n",
        "        if np.any(np.diff(cuts) <= 0):\n",
        "            continue\n",
        "        labels = np.digitize(y1d, bins=cuts).astype(int)\n",
        "        counts = Counter(labels)\n",
        "        if all(c >= min_per_class for c in counts.values()) and len(counts) > 1:\n",
        "            return labels\n",
        "    return None\n",
        "\n",
        "def stratify_from_df_or_y(df_labels, idx, y_subset, col='NIVEL_C'):\n",
        "    \"\"\"Intenta usar df[col] como etiqueta; si falla, usa percentiles en y_subset.\"\"\"\n",
        "    try:\n",
        "        ycat_full = df_labels.loc[:, col].values.astype(int)\n",
        "        ycat = ycat_full[idx]\n",
        "        c10 = Counter(ycat)\n",
        "        if all(v >= 2 for v in c10.values()) and len(c10) > 1:\n",
        "            return ycat\n",
        "    except Exception:\n",
        "        pass\n",
        "    return make_strat_labels(y_subset[:,0], n_bins=3, min_per_class=2)\n",
        "\n",
        "def split_subset(X, y, df_labels=None, n_sub=1000, test_size=0.20, seed=42):\n",
        "    \"\"\"\n",
        "    1) Toma un subset aleatorio de tamaño n_sub.\n",
        "    2) Escala y (MinMax) sobre el subset.\n",
        "    3) Split train/test con estratificación si es viable.\n",
        "    4) Split train/valid (20% del train), con re-estratificación si es posible.\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(seed)\n",
        "    n_total = X.shape[0]\n",
        "    n_sub = min(n_sub, n_total)\n",
        "    idx_sub = rng.choice(n_total, size=n_sub, replace=False)\n",
        "\n",
        "    X_sub = X[idx_sub]\n",
        "    y_sub = y[idx_sub]\n",
        "    # etiquetas auxiliares para estratificación\n",
        "    ycat_sub = stratify_from_df_or_y(df_labels, idx_sub, y_sub) if df_labels is not None else make_strat_labels(y_sub[:,0])\n",
        "    # escalar objetivo en el subset\n",
        "    scaler = MinMaxScaler()\n",
        "    y_sub_scaled = scaler.fit_transform(y_sub)\n",
        "\n",
        "    split_kwargs = dict(test_size=test_size, random_state=seed, shuffle=True)\n",
        "    if ycat_sub is not None:\n",
        "        X_tr, X_te, y_tr, y_te, ycat_tr, ycat_te = train_test_split(\n",
        "            X_sub, y_sub_scaled, ycat_sub, stratify=ycat_sub, **split_kwargs\n",
        "        )\n",
        "    else:\n",
        "        X_tr, X_te, y_tr, y_te = train_test_split(X_sub, y_sub_scaled, **split_kwargs)\n",
        "        ycat_tr = ycat_te = None\n",
        "\n",
        "    # Validación (20% del train)\n",
        "    if ycat_tr is not None:\n",
        "        y_tr_raw = y_tr[:,0]\n",
        "        ycat_t = make_strat_labels(y_tr_raw, n_bins=3, min_per_class=2)\n",
        "        if ycat_t is not None:\n",
        "            X_tr, X_va, y_tr, y_va, ycat_tr, ycat_va = train_test_split(\n",
        "                X_tr, y_tr, ycat_tr, test_size=0.20, random_state=seed, stratify=ycat_t\n",
        "            )\n",
        "        else:\n",
        "            X_tr, X_va, y_tr, y_va = train_test_split(\n",
        "                X_tr, y_tr, test_size=0.20, random_state=seed, shuffle=True\n",
        "            )\n",
        "            ycat_va = None\n",
        "    else:\n",
        "        X_tr, X_va, y_tr, y_va = train_test_split(\n",
        "            X_tr, y_tr, test_size=0.20, random_state=seed, shuffle=True\n",
        "        )\n",
        "        ycat_va = None\n",
        "\n",
        "    # Reporte rápido\n",
        "    print(\"Originales (conservados):\", X_orig.shape, y_orig.shape)\n",
        "    print(f\"Subset de {n_sub}:\", X_sub.shape, y_sub.shape)\n",
        "    print(\"Train/Valid/Test:\", X_tr.shape, X_va.shape, X_te.shape)\n",
        "    if ycat_sub is not None:\n",
        "        print(\"Distribución clases subset:\", Counter(ycat_sub))\n",
        "\n",
        "    return {\n",
        "        \"idx_sub\": idx_sub,\n",
        "        \"X_train\": X_tr, \"X_valid\": X_va, \"X_test\": X_te,\n",
        "        \"y_train\": y_tr, \"y_valid\": y_va, \"y_test\": y_te\n",
        "    }\n",
        "from copy import deepcopy\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "def make_tabnet(cat_info, params):\n",
        "    cat_idxs = [i for i, f in enumerate(features) if f in CATEGORICAL_COLUMNS]\n",
        "    cat_dims = [categorical_dims[f] for f in features if f in CATEGORICAL_COLUMNS]\n",
        "    cat_emb_dim = [min(params['emb'], max(4, (dim + 1)//2)) for dim in cat_dims]\n",
        "    return cat_idxs, cat_dims, cat_emb_dim\n",
        "\n",
        "def build_optimizer(optimizer_type, learning_rate, momentum, weight_decay):\n",
        "    if optimizer_type == 'adam':\n",
        "        optimizer_fn = torch.optim.Adam\n",
        "        optimizer_params = {'lr': learning_rate, 'weight_decay': weight_decay}\n",
        "    elif optimizer_type == 'adamw':\n",
        "        optimizer_fn = torch.optim.AdamW\n",
        "        optimizer_params = {'lr': learning_rate, 'weight_decay': weight_decay}\n",
        "    elif optimizer_type == 'sgd':\n",
        "        optimizer_fn = torch.optim.SGD\n",
        "        optimizer_params = {'lr': learning_rate, 'momentum': momentum, 'weight_decay': weight_decay}\n",
        "    elif optimizer_type == 'rmsprop':\n",
        "        optimizer_fn = torch.optim.RMSprop\n",
        "        optimizer_params = {'lr': learning_rate, 'momentum': momentum, 'weight_decay': weight_decay}\n",
        "    return optimizer_fn, optimizer_params\n",
        "def objective_regression(trial):\n",
        "    # Capacidad TabNet\n",
        "    n_d     = trial.suggest_int('n_d', 2, 256)\n",
        "    n_a     = trial.suggest_int('n_a', 2, 256)\n",
        "    n_steps = trial.suggest_int('n_steps', 1, 10)\n",
        "\n",
        "    gamma         = trial.suggest_float('gamma', 1e-12, 1e2)\n",
        "    lambda_sparse = trial.suggest_float('lambda_sparse', 1e-12, 1e2, log=True)\n",
        "\n",
        "    batch_size  = trial.suggest_categorical('batch_size', [1024, 2048, 4096])\n",
        "    mask_type   = trial.suggest_categorical('mask_type', ['entmax', 'sparsemax'])\n",
        "    emb         = trial.suggest_int('emb', 3, 70)\n",
        "\n",
        "    momentum      = trial.suggest_float('momentum', 0.001, 0.9)\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-7, 963e-1, log=True)\n",
        "    weight_decay  = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
        "\n",
        "    scheduler_gamma = trial.suggest_float('scheduler_gamma', 0.1, 0.99)\n",
        "    step_size       = trial.suggest_int('step_size',  1, 20)\n",
        "\n",
        "    virtual_batch_size = trial.suggest_categorical('virtual_batch_size', [256,512,1024])\n",
        "    if isinstance(batch_size, int) and isinstance(virtual_batch_size, int) and virtual_batch_size > batch_size:\n",
        "        virtual_batch_size = batch_size // 2 if batch_size >= 64 else batch_size\n",
        "\n",
        "    optimizer_type = trial.suggest_categorical('optimizer_type', ['adam', 'adamw', 'sgd', 'rmsprop'])\n",
        "    optimizer_fn, optimizer_params = build_optimizer(optimizer_type, learning_rate, momentum, weight_decay)\n",
        "\n",
        "    p   = trial.suggest_float('p', 1e-6, 0.99)\n",
        "    aug = RegressionSMOTE(p=p)\n",
        "\n",
        "    cat_idxs = [i for i, f in enumerate(features) if f in CATEGORICAL_COLUMNS]\n",
        "    cat_dims = [categorical_dims[f] for f in features if f in CATEGORICAL_COLUMNS]\n",
        "    cat_emb_dim = [min(emb, max(4, (dim + 1)//2)) for dim in cat_dims]\n",
        "\n",
        "    model = CustomTabNetRegressor(\n",
        "        cat_dims=cat_dims, cat_emb_dim=cat_emb_dim, cat_idxs=cat_idxs,\n",
        "        n_d=n_d, n_a=n_a, n_steps=n_steps, gamma=gamma, lambda_sparse=lambda_sparse,\n",
        "        mask_type=mask_type, optimizer_fn=optimizer_fn, optimizer_params=optimizer_params,\n",
        "        scheduler_params={\"gamma\": scheduler_gamma, \"step_size\": step_size},\n",
        "        scheduler_fn=torch.optim.lr_scheduler.StepLR, verbose=False\n",
        "    )\n",
        "    model.fit(\n",
        "        X_train=X_train, y_train=y_train,\n",
        "        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
        "        eval_name=['train', 'valid'],\n",
        "        eval_metric=['mae'],\n",
        "        loss_fn=my_r2_score_fn,  # (conserva tu lógica)\n",
        "        max_epochs=70, patience=40,\n",
        "        batch_size=batch_size, virtual_batch_size=virtual_batch_size,\n",
        "        num_workers=1, drop_last=False, augmentations=aug,\n",
        "    )\n",
        "    mae = model.history['loss'][-1]\n",
        "    return mae\n",
        "\n",
        "def eval_and_print(title, clf_model, X_test, y_test):\n",
        "    \"\"\"Evalúa R² en escala original (inverse_transform) y lo imprime.\"\"\"\n",
        "    y_pred = clf_model.predict(X_test)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    print(f\"{title}: R2={r2:.4f}\")\n",
        "    return r2\n",
        "\n",
        "def run_three_training_strategies(\n",
        "    # modelos / kwargs\n",
        "    clf_base,                   # modelo ya entrenado en la Fase 1 (con warm_start=True)\n",
        "    model_init_kwargs,          # dict con los kwargs para construir un modelo nuevo idéntico (desde cero)\n",
        "    # datos antiguos (Fase 1)\n",
        "    X_train_old, y_train_old,   # típicamente (X_train, y_train[:,0:1]) de los 1000\n",
        "    X_test_old, y_test_old,  # test y scaler usados en la Fase 1\n",
        "    # datos nuevos (Fase 2)\n",
        "    X_tr_new, y_tr_new,         # train de los 500\n",
        "    X_va_new, y_va_new,         # valid de los 500 (para early stopping)\n",
        "    X_te_new, y_te_new,  # test nuevo y su scaler\n",
        "    # entrenamiento\n",
        "    batch_size, virtual_batch_size, aug,\n",
        "    max_epochs_ft_inc=200, patience_ft_inc=70,\n",
        "    max_epochs_ft_new=200, patience_ft_new=70,\n",
        "    max_epochs_scratch=200, patience_scratch=70,\n",
        "    lower_lr_factor=0.1, min_lr=1e-5\n",
        "):\n",
        "    \"\"\"\n",
        "    Ejecuta:\n",
        "      A) Fine-tuning incremental (old + new)\n",
        "      B) Fine-tuning no incremental (solo new)\n",
        "      C) Desde cero (old + new)\n",
        "    y evalúa R² en test viejo y test nuevo (ambos en escala original).\n",
        "    Devuelve un dict con los R².\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "\n",
        "    # =============================\n",
        "    # A) Fine-tuning incremental\n",
        "    # =============================\n",
        "    clf_ft_inc = deepcopy(clf_base)  # copia del clf ya entrenado\n",
        "    # bajar LR para fine-tune (opcional, recomendado)\n",
        "    if hasattr(clf_ft_inc, \"_optimizer\"):\n",
        "        for g in clf_ft_inc._optimizer.param_groups:\n",
        "            g[\"lr\"] = max(g[\"lr\"] * lower_lr_factor, min_lr)\n",
        "\n",
        "    X_inc = np.concatenate([X_train_old, X_tr_new], axis=0)\n",
        "    y_inc = np.concatenate([y_train_old, y_tr_new], axis=0)\n",
        "\n",
        "    clf_ft_inc.fit(\n",
        "        X_train=X_inc, y_train=y_inc,\n",
        "        eval_set=[(X_inc, y_inc), (X_va_new, y_va_new)],\n",
        "        eval_name=['train_inc', 'valid_new'],\n",
        "        eval_metric=['mae'], loss_fn=my_r2_score_fn,\n",
        "        max_epochs=max_epochs_ft_inc, patience=patience_ft_inc,\n",
        "        batch_size=batch_size, virtual_batch_size=virtual_batch_size,\n",
        "        num_workers=1, drop_last=False, augmentations=aug,\n",
        "    )\n",
        "\n",
        "    print(\"\\n== Desempeño: Fine-tuning incremental ==\")\n",
        "    r2_old_inc = eval_and_print(\"Test viejo (FT incremental)\", clf_ft_inc, X_test_old, y_test_old)\n",
        "    r2_new_inc = eval_and_print(\"Test nuevo (FT incremental)\", clf_ft_inc, X_te_new,  y_te_new)\n",
        "    results[\"fine_tune_incremental\"] = {\"R2_old_test\": r2_old_inc, \"R2_new_test\": r2_new_inc, \"model\": clf_ft_inc}\n",
        "\n",
        "    # =============================\n",
        "    # B) Fine-tuning no incremental (solo nuevos)\n",
        "    # =============================\n",
        "    clf_ft_new = deepcopy(clf_base)\n",
        "    if hasattr(clf_ft_new, \"_optimizer\"):\n",
        "        for g in clf_ft_new._optimizer.param_groups:\n",
        "            g[\"lr\"] = max(g[\"lr\"] * lower_lr_factor, min_lr)\n",
        "\n",
        "    clf_ft_new.fit(\n",
        "        X_train=X_tr_new, y_train=y_tr_new,\n",
        "        eval_set=[(X_tr_new, y_tr_new), (X_va_new, y_va_new)],\n",
        "        eval_name=['train_new', 'valid_new'],\n",
        "        eval_metric=['mae'], loss_fn=my_r2_score_fn,\n",
        "        max_epochs=max_epochs_ft_new, patience=patience_ft_new,\n",
        "        batch_size=batch_size, virtual_batch_size=virtual_batch_size,\n",
        "        num_workers=1, drop_last=False, augmentations=aug,\n",
        "    )\n",
        "\n",
        "    print(\"\\n== Desempeño: Fine-tuning NO incremental (solo nuevos) ==\")\n",
        "    r2_old_new = eval_and_print(\"Test viejo (FT no incremental)\", clf_ft_new, X_test_old, y_test_old)\n",
        "    r2_new_new = eval_and_print(\"Test nuevo (FT no incremental)\", clf_ft_new, X_te_new,  y_te_new)\n",
        "    results[\"fine_tune_only_new\"] = {\"R2_old_test\": r2_old_new, \"R2_new_test\": r2_new_new, \"model\": clf_ft_new}\n",
        "\n",
        "    # =============================\n",
        "    # C) Desde cero (cumulative old+new)\n",
        "    # =============================\n",
        "    # model_init_kwargs debe contener todo lo necesario para reconstruir el TabNet\n",
        "    clf_scratch = CustomTabNetRegressor(**model_init_kwargs)\n",
        "\n",
        "    X_cum = np.concatenate([X_train_old, X_tr_new], axis=0)\n",
        "    y_cum = np.concatenate([y_train_old, y_tr_new], axis=0)\n",
        "\n",
        "    clf_scratch.fit(\n",
        "        X_train=X_cum, y_train=y_cum,\n",
        "        eval_set=[(X_cum, y_cum), (X_va_new, y_va_new)],\n",
        "        eval_name=['train_cum', 'valid_new'],\n",
        "        eval_metric=['mae'], loss_fn=my_r2_score_fn,\n",
        "        max_epochs=max_epochs_scratch, patience=patience_scratch,\n",
        "        batch_size=batch_size, virtual_batch_size=virtual_batch_size,\n",
        "        num_workers=1, drop_last=False, augmentations=aug,\n",
        "    )\n",
        "\n",
        "    print(\"\\n== Desempeño: Desde cero (old+new) ==\")\n",
        "    r2_old_sc = eval_and_print(\"Test viejo (desde cero)\", clf_scratch, X_test_old, y_test_old)\n",
        "    r2_new_sc = eval_and_print(\"Test nuevo (desde cero)\", clf_scratch, X_te_new,  y_te_new)\n",
        "    results[\"from_scratch\"] = {\"R2_old_test\": r2_old_sc, \"R2_new_test\": r2_new_sc, \"model\": clf_scratch}\n",
        "    return results\n",
        "def pick_new_indices(n_new=500, seed=123):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    universe = np.setdiff1d(np.arange(X.shape[0]), splits_1000[\"idx_sub\"], assume_unique=True)\n",
        "    n_new = min(n_new, universe.shape[0])\n",
        "    return rng.choice(universe, size=n_new, replace=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Xdata = df = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Diego/SuperEventos_Criticidad_AguasAbajo_CODEs.pkl')\n",
        "Xdata = Xdata[Xdata['duracion_h'] <= 100]\n",
        "# ---------------------------------------------------------\n",
        "# Etapa 1: seleccionar objetivo (SAIDI o SAIFI) con forma (N,1)\n",
        "# Extraer variables objetivo\n",
        "Dur_h = Xdata['duracion_h'].values\n",
        "SAIDI = Xdata['SAIDI'].values\n",
        "df1=Xdata.copy()\n",
        "# Eliminar columnas no utilizadas\n",
        "Xdata.drop(['inicio_evento', 'h0-solar_rad', 'h0-uv', 'h1-solar_rad', 'h1-uv', 'h2-solar_rad', 'h2-uv', 'h3-solar_rad', 'h3-uv',\n",
        "            'h4-solar_rad', 'h4-uv', 'h5-solar_rad', 'h5-uv', 'h19-solar_rad', 'h19-uv', 'h20-solar_rad', 'h20-uv',\n",
        "            'h21-solar_rad', 'h21-uv', 'h22-solar_rad', 'h22-uv', 'h23-solar_rad', 'h23-uv', 'evento', 'fin', 'inicio',\n",
        "            'cnt_usus', 'DEP', 'MUN', 'FECHA', 'NIVEL_C', 'VALOR_C', 'TRAMOS_AGUAS_ABAJO', 'EQUIPOS_PUNTOS',\n",
        "            'PUNTOS_POLIGONO', 'LONGITUD2', 'LATITUD2', 'FECHA_C','TRAMOS_AGUAS_ABAJO_CODES','ORDER_'],\n",
        "           inplace=True, axis=1)\n",
        "\n",
        "# Definir la variable objetivo y eliminarla del conjunto de características\n",
        "target = ['SAIFI', 'SAIDI', 'duracion_h']\n",
        "y1 = Xdata[target].values\n",
        "Xdata.drop(target, axis=1, inplace=True)\n",
        "y = y1[:, 0:1].astype('float32')\n",
        "\n",
        "# Copia de trabajo de X\n",
        "df = Xdata.copy()\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Etapa 2: tipificar columnas\n",
        "NUMERIC_COLUMNS = df.select_dtypes(include=['number']).columns.tolist()\n",
        "CATEGORICAL_COLUMNS = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Etapa 3: imputación numérica\n",
        "max_values = {}\n",
        "for col in NUMERIC_COLUMNS:\n",
        "    max_value = pd.to_numeric(df[col], errors='coerce').max()\n",
        "    if pd.isna(max_value):\n",
        "        max_value = 0.0\n",
        "    max_values[col] = max_value\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(-10.0 * max_value)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Etapa 4: codificación categórica\n",
        "label_encoders = {}\n",
        "categorical_dims = {}\n",
        "for col in CATEGORICAL_COLUMNS:\n",
        "    enc = LabelEncoder()\n",
        "    s = df[col].astype(str).fillna(\"no aplica\")\n",
        "    enc.fit(s)\n",
        "    df[col] = enc.transform(s)\n",
        "    label_encoders[col] = enc\n",
        "    categorical_dims[col] = len(enc.classes_)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Etapa 5: construir matrices X, y\n",
        "unused_feat = []\n",
        "# Si Xdata NO incluye el target, basta con tomar todas las columnas\n",
        "features = [c for c in df.columns if c not in unused_feat]\n",
        "X = df[features].values.astype('float32')\n",
        "# Etapa 6: clases auxiliares para estratificación\n",
        "try:\n",
        "    # usar etiqueta externa si existe\n",
        "    y_categorized = df1['NIVEL_C'].values.astype(int)\n",
        "except Exception:\n",
        "    # fallback: terciles del objetivo\n",
        "    percentiles = np.percentile(y[:, 0], [33.33, 66.66])\n",
        "    y_categorized = np.digitize(y[:, 0].flatten(), bins=percentiles).astype(int)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Etapa 7: escalar objetivo (regresión)\n",
        "scaler = MinMaxScaler()\n",
        "y_scaled = scaler.fit_transform(y)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Etapa 8: split train/test estratificado\n",
        "X_train, X_test, y_train, y_test, ycat_train, ycat_test = train_test_split(\n",
        "    X, y_scaled, y_categorized, test_size=0.20, random_state=42, stratify=y_categorized\n",
        ")\n",
        "\n",
        "# Etapa 8b: split train/valid estratificado por percentiles de y_train\n",
        "percentiles_t = np.percentile(y_train[:, 0], [25, 50, 75])\n",
        "y_categorized_t = np.digitize(y_train[:, 0].flatten(), bins=percentiles_t).astype(int)\n",
        "\n",
        "X_train, X_valid, y_train, y_valid, ycat_train, ycat_valid = train_test_split(\n",
        "    X_train, y_train, ycat_train, test_size=0.20, random_state=42, stratify=y_categorized_t\n",
        ")\n",
        "\n",
        "# Comprobaciones rápidas\n",
        "print(X.shape, y.shape)\n",
        "print(\"Train/Valid/Test:\", X_train.shape, X_valid.shape, X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGiAvVg14SM0",
        "outputId": "257329d9-02d6-4d14-fe1e-7eb759574189"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(166323, 326) (166323, 1)\n",
            "Train/Valid/Test: (106446, 326) (26612, 326) (33265, 326)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "par={'n_d': 75, 'n_a': 253, 'n_steps': 10, 'gamma': 77.13726204291119, 'lambda_sparse': 2.033600968988791e-11,\n",
        "     'batch_size': 2048, 'mask_type': 'sparsemax', 'emb': 41, 'momentum': 0.5158252222355557,\n",
        "     'learning_rate': 0.0021040765227438576, 'weight_decay': 8.376844290145493e-06,\n",
        "     'scheduler_gamma': 0.796171403854875, 'step_size': 3, 'virtual_batch_size': 512,\n",
        "     'optimizer_type': 'rmsprop', 'p': 0.14330200935617907}"
      ],
      "metadata": {
        "id": "UmZf0w2h32wZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parámetros obtenidos del estudio\n",
        "# Parámetros obtenidos del estudio\n",
        "best_params =  par\n",
        "\n",
        "# Asignación de parámetros\n",
        "n_d = best_params['n_d']\n",
        "n_a = best_params['n_a']\n",
        "n_steps = best_params['n_steps']\n",
        "gamma = best_params['gamma']\n",
        "lambda_sparse = best_params['lambda_sparse']\n",
        "mask_type = best_params['mask_type']\n",
        "batch_size = best_params['batch_size']\n",
        "emb = best_params['emb']\n",
        "p = best_params['p']\n",
        "momentum = best_params['momentum']\n",
        "learning_rate = best_params['learning_rate']\n",
        "weight_decay = best_params['weight_decay']\n",
        "scheduler_gamma = best_params['scheduler_gamma']\n",
        "step_size = best_params['step_size']\n",
        "virtual_batch_size = best_params['virtual_batch_size']\n",
        "optimizer_type = best_params['optimizer_type']\n",
        "\n",
        "if optimizer_type == 'adam':\n",
        "    optimizer_fn = torch.optim.Adam\n",
        "    optimizer_params = {'lr': learning_rate, 'weight_decay': weight_decay}\n",
        "elif optimizer_type == 'adamw':\n",
        "    optimizer_fn = torch.optim.AdamW\n",
        "    optimizer_params = {'lr': learning_rate, 'weight_decay': weight_decay}\n",
        "elif optimizer_type == 'sgd':\n",
        "    optimizer_fn = torch.optim.SGD\n",
        "    optimizer_params = {'lr': learning_rate, 'momentum': momentum, 'weight_decay': weight_decay}\n",
        "elif optimizer_type == 'rmsprop':\n",
        "    optimizer_fn = torch.optim.RMSprop\n",
        "    optimizer_params = {'lr': learning_rate, 'momentum': momentum, 'weight_decay': weight_decay}\n",
        "\n",
        "# Definición de la técnica de aumento de datos\n",
        "aug = RegressionSMOTE(p=p)\n",
        "\n",
        "# Identificación de columnas categóricas y sus dimensiones\n",
        "cat_idxs = [i for i, f in enumerate(features) if f in CATEGORICAL_COLUMNS]\n",
        "cat_dims = [categorical_dims[f] for f in features if f in CATEGORICAL_COLUMNS]\n",
        "cat_emb_dim = [min(emb, (dim + 1) // 2) for dim in cat_dims]\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# Valores de lambda_sparse para iterar\n",
        "lambda_values = [3.2e-11, 3.2e-7, 3.2e-5, lambda_sparse, 3.2e-1, 9e-1]\n",
        "\n",
        "# Listas para guardar las máscaras y matrices de explicación de cada iteración\n",
        "all_masks = []\n",
        "all_explain_matrices = []\n",
        "iteration=2\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "for iteration, lambda_sparse in tqdm(enumerate(lambda_values), total=len(lambda_values), desc=\"Entrenando modelos\"):\n",
        "    print(f\"Iteración {iteration + 1}, lambda_sparse={lambda_sparse}\")\n",
        "\n",
        "    # Aquí va todo tu bloque de entrenamiento...\n",
        "    clf = CustomTabNetRegressor(\n",
        "        cat_dims=cat_dims,\n",
        "        cat_emb_dim=cat_emb_dim,\n",
        "        cat_idxs=cat_idxs,\n",
        "        n_d=n_d,\n",
        "        n_a=n_a,\n",
        "        n_steps=n_steps,\n",
        "        gamma=gamma,\n",
        "        lambda_sparse=lambda_sparse,\n",
        "        mask_type=mask_type,\n",
        "        optimizer_fn=optimizer_fn,\n",
        "        optimizer_params=optimizer_params,\n",
        "        scheduler_params={\"gamma\": scheduler_gamma, \"step_size\": step_size},\n",
        "        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "        momentum=momentum,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    clf.fit(\n",
        "        X_train=X_train, y_train=y_train[:,0:1],\n",
        "        eval_set=[(X_train, y_train[:,0:1]), (X_valid, y_valid[:,0:1])],\n",
        "        eval_name=['train', 'valid'], eval_metric=['mae'],\n",
        "        loss_fn=my_r2_score_fn,\n",
        "        max_epochs=200, patience=70,\n",
        "        batch_size=batch_size, virtual_batch_size=virtual_batch_size,\n",
        "        num_workers=1, drop_last=False, augmentations=aug,\n",
        "    )\n",
        "\n",
        "    _, masks = clf.explain(X, normalize=True)\n",
        "    savepath='/content/drive/MyDrive/Colab Notebooks/Diego/'\n",
        "    np.save(f\"{savepath}masks_iteration_{iteration+1}.npy\", list(masks.values()), allow_pickle=True)\n",
        "    del _, masks\n",
        "    print(\"¡Proceso completo! Las máscaras y matrices de explicación se han guardado correctamente.\")\n"
      ],
      "metadata": {
        "id": "aKnXo3CNUsM7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28eff7ab-28a3-4e25-9847-65cbbfd39adf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEntrenando modelos:   0%|          | 0/6 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 1, lambda_sparse=3.2e-11\n",
            "\n",
            "Early stopping occurred at epoch 184 with best_epoch = 114 and best_valid_mae = 0.00106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "Entrenando modelos:  17%|█▋        | 1/6 [27:57<2:19:45, 1677.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Proceso completo! Las máscaras y matrices de explicación se han guardado correctamente.\n",
            "Iteración 2, lambda_sparse=3.2e-07\n",
            "\n",
            "Early stopping occurred at epoch 140 with best_epoch = 70 and best_valid_mae = 0.00088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "Entrenando modelos:  33%|███▎      | 2/6 [49:54<1:37:42, 1465.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Proceso completo! Las máscaras y matrices de explicación se han guardado correctamente.\n",
            "Iteración 3, lambda_sparse=3.2e-05\n",
            "Stop training because you reached max_epochs = 200 with best_epoch = 191 and best_valid_mae = 0.00085\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "Entrenando modelos:  50%|█████     | 3/6 [1:20:30<1:21:44, 1634.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Proceso completo! Las máscaras y matrices de explicación se han guardado correctamente.\n",
            "Iteración 4, lambda_sparse=2.033600968988791e-11\n",
            "\n",
            "Early stopping occurred at epoch 167 with best_epoch = 97 and best_valid_mae = 0.00101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "Entrenando modelos:  67%|██████▋   | 4/6 [1:46:29<53:29, 1604.70s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Proceso completo! Las máscaras y matrices de explicación se han guardado correctamente.\n",
            "Iteración 5, lambda_sparse=0.32\n",
            "\n",
            "Early stopping occurred at epoch 131 with best_epoch = 61 and best_valid_mae = 0.00105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "Entrenando modelos:  83%|████████▎ | 5/6 [2:07:09<24:33, 1473.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Proceso completo! Las máscaras y matrices de explicación se han guardado correctamente.\n",
            "Iteración 6, lambda_sparse=0.9\n",
            "\n",
            "Early stopping occurred at epoch 167 with best_epoch = 97 and best_valid_mae = 0.00162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "Entrenando modelos: 100%|██████████| 6/6 [2:33:00<00:00, 1530.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Proceso completo! Las máscaras y matrices de explicación se han guardado correctamente.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}